{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_08_Running_Spark_on_a_cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n8JD51WVauRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 13:40:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/26 13:40:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 08 - Running Spark on a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q4ebRam_m3Z"
   },
   "source": [
    "## Running a Spark program\n",
    "\n",
    "### Command `spark-submit`\n",
    "\n",
    "-   Submits a Spark program (an application) to a cluster\n",
    "-   More specifically, it launches the driver program and invokes the main() method specified by the user\n",
    "\n",
    "-   Examples:\n",
    "```sh\n",
    "$ bin/spark-submit --master yarn --deploy-mode cluster \\  \n",
    "     --py-files anotherlib.zip,anotherfile.py \\  \n",
    "     --num-executors 10 --executor-cores 2 \\  \n",
    "     my-script.py script_options\n",
    "```\n",
    "\n",
    "### `spark-submit` options\n",
    "\n",
    "-   `master`: cluster manager to use (options: `yarn`, `mesos://host:port`, `spark://host:port`, `local[n]`)\n",
    "\n",
    "-   `deploy-mode`: Two ways of deploying\n",
    "\n",
    "    -   `client`: runs the driver on the local node \n",
    "\n",
    "    -   `cluster`: runs the driver on a node of the cluster\n",
    "\n",
    "-   `class`: class to execute (Java or Scala)\n",
    "\n",
    "-   `name`: name of the application (it will be shown in the Spark web)\n",
    "\n",
    "-   `jars`: jar files to add to the classpath (Java o Scala)\n",
    "\n",
    "-   `py-files`: files to add to the PYTHONPATH (`.py`,`.zip`,`.egg`)\n",
    "\n",
    "-   `files`: data files for the applications \n",
    "\n",
    "-   `executor-memory`: total memory of each executor\n",
    "\n",
    "-   `driver-memory`: memory of the driver process\n",
    "\n",
    "For more options: `spark-submit --help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fr0Lfj_3_3Xy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: spark/bin/spark-submit: not found\n",
      "22/12/26 14:00:14 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from a89ada86b54a:34791 in 10000 milliseconds\n",
      "22/12/26 14:01:33 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 166806 ms exceeds timeout 120000 ms\n",
      "22/12/26 14:00:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "22/12/26 14:02:58 WARN NettyRpcEnv: Ignored message: true\n",
      "22/12/26 14:03:09 WARN NettyRpcEnv: Ignored message: true\n",
      "22/12/26 14:03:19 WARN NettyRpcEnv: Ignored message: true\n",
      "22/12/26 14:02:47 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n",
      "22/12/26 14:03:21 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from a89ada86b54a:34791 in 120 seconds\n",
      "22/12/26 14:03:21 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(true)\n",
      "22/12/26 14:03:21 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "!spark/bin/spark-submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MBC4c6_AgWb"
   },
   "source": [
    "![Spark-YARN: Client mode](https://i.pinimg.com/originals/e5/e6/a6/e5e6a6dbc4da4a2dbc1b54effd5995ee.jpg)\n",
    "\n",
    "\n",
    "![Spark-on-YARN: Cluster mode](https://i.pinimg.com/originals/db/16/e9/db16e98baed2a9b54c64e931e1f9b2b5.jpg)\n",
    "\n",
    "Source: [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0CiVeetECWE"
   },
   "source": [
    "## Configuration parameters\n",
    "\n",
    "Several parameters that can be adjusted in runtime\n",
    "\n",
    "-   In the script\n",
    "```python\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"My app\")\n",
    "conf.set(\"spark.master\", \"local[2]\") # Cluster manager local mode with 2 threads\n",
    "conf.set(\"spark.ui.port\", \"3600\")    # Port of the Spark web interface (by default: 4040)\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "-   Using flags in the `spark-submit` command\n",
    "```sh\n",
    "$ bin/spark-submit --master local[2] --name \"My app\" \\  \n",
    "    --conf spark.ui.port=3600 my-script.py\n",
    "```    \n",
    "    \n",
    "-   Using a properties file \n",
    "```sh\n",
    "$ cat config.conf\n",
    "spark.master     local[2] \n",
    "spark.app.name   \"My app\" \n",
    "spark.ui.port 3600\n",
    "$ bin/spark-submit --properties-file config.conf my-script.py\n",
    "```\n",
    "\n",
    "More information: <http://spark.apache.org/docs/latest/configuration.html#spark-properties>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKwBitASEZrX"
   },
   "source": [
    "## Example: Python script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wK5ru0Z9JpJO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /myscript.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat \"$DRIVE_DATA\"/myscript.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFuf5kOLCrnB"
   },
   "outputs": [],
   "source": [
    "# NOTE: It won't work in a notebook.\n",
    "# Do NOT modify the following line\n",
    "cat << EOF > /tmp/myscript.py\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "def main():\n",
    "    conf = SparkConf()\n",
    "    conf.set(\"spark.app.name\", \"My Python script\")\n",
    "\n",
    "    # Initialise the SparkContext\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"FATAL\")\n",
    "\n",
    "    rdd = sc.parallelize(range(100000)).cache()\n",
    "    \n",
    "    rdd2 = rdd.map(lambda x: (x, 2*x))\\\n",
    "              .map(lambda (x,y): (x-100, y**2))\\\n",
    "              .reduceByKey(lambda x,y: x+y)\\\n",
    "              .values()\n",
    "               \n",
    "    r = rdd2.reduce(add)\n",
    "    \n",
    "    print(\"Final result = {0}\".format(r))\n",
    "    \n",
    "    # Stop the SparkContext\n",
    "    sc.stop()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLVf_t_RMnM6"
   },
   "outputs": [],
   "source": [
    "!spark/bin/spark-submit --master local[8] \"$DRIVE_DATA\"myscript.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
