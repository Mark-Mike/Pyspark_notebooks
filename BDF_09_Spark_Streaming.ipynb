{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_09_Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n8JD51WVauRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 13:43:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/26 13:43:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/26 13:43:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 09 - Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trU_wEl7Og5p"
   },
   "source": [
    "-   Scalable, *high-throughput*, fault-tolerant streaming processing\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-HJ8x45gN3kE/WNCULL6J6eI/AAAAAAAAAZ0/1LfYt5IwE3sINSxWunqTrbqyrm7irZTCwCEw/s1600/spark.JPG\" alt=\"Spark Streaming flow\" style=\"width: 900px;\"/>\n",
    "\n",
    "-   Input from multiple sources: Kafka, Flume, Twitter, ZeroMQ, Kinesis or sockets TCP\n",
    "\n",
    "\n",
    "## Spark Streaming APIs \n",
    "\n",
    "- DStream API\n",
    "      - Original API, based on RDDs\n",
    "- Structured Streaming\n",
    "      - Available from version 2.2, based on DataFrames\n",
    "\n",
    "\n",
    "Spark Streaming page: <https://spark.apache.org/streaming/>\n",
    "Main documentation (last version): <https://spark.apache.org/docs/latest/streaming-programming-guide.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of20h1csKttE"
   },
   "source": [
    "\n",
    "## DStream API\n",
    "\n",
    "Main abstraction: DStream (`discretized stream`).\n",
    "\n",
    "-   Represents a continuous data stream\n",
    "\n",
    "![dstreams](http://persoal.citius.usc.es/tf.pena/TCDM/figs/dstreams.png)\n",
    "\n",
    "*Micro-batch* architecture\n",
    "\n",
    "-   Received data are grouped into batches\n",
    "-   Batches are created at regular intervals (batch interval)\n",
    "-   Every batch has the form of an RDD, which can be processed by Spark\n",
    "-   In addition, stateful transformations can be performed by\n",
    "    -   Window operations\n",
    "    -   Tracking of each key state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLHkK1rZMiTH"
   },
   "source": [
    "### DStream example: stateless online WordCount example\n",
    "\n",
    "To run the example:\n",
    "\n",
    "   1. In a terminal, access the Docker container with `docker exec -ti container_id /bin/bash` (run `docker ps` to know the container_id)\n",
    "   2. Once in the container's terminal, use netcat as a server in the port 9999\n",
    "\n",
    "    `$ nc -lk 9999`\n",
    "\n",
    "   2. Run here the PySpark code that you will find below\n",
    "\n",
    "   3. Write text lines in netcat's terminal. They will be picked up and processed by the script\n",
    "    - Write repeated words, to make sure they are counted right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ycdjim6PN9Oc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: sudo: not found\n"
     ]
    }
   ],
   "source": [
    "#!sudo apt-get update && sudo apt-get install netcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ev0hpG2aOxi7"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnohup nc -lk 9999 &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py:617\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"Call the given cmd in a subprocess, piping stdout/err\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    other than simple text.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "#!nohup nc -lk 9999 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8Bg2GybM06T"
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from operator import add\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# Streaming context with a batch interval of 5 sec.\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# DStream that connects to localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Run a WordCount\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(add)\n",
    "              \n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Start the computation\n",
    "ssc.awaitTerminationOrTimeout(60) # Wait for it to finish (stops in 60 seconds)\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W5bT_xmNAwB"
   },
   "source": [
    "### DStream example: stateful online WordCount example\n",
    "\n",
    "Repeat the previous steps running the following code\n",
    "\n",
    " - Check that the number of words is accumulated between accesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2i1wCu9NKJn"
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from operator import add\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# Streaming context with a batch interval of 5 sec.\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# DStream that connects to localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "ssc.checkpoint(\"/tmp/cpdir\") # Enables checkpoint\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .updateStateByKey(updateFunc)\n",
    "\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Start the computation\n",
    "ssc.awaitTerminationOrTimeout(60) # Wait for it to finish (stops in 60 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFwgHSUDNPUZ"
   },
   "source": [
    "## Structured Streaming \n",
    "\n",
    "Utilises the structured API (DataFrames, DataSets and SQL)\n",
    "\n",
    "- As they arrive to the system, it reads data, processes them and adds them to a DataFrame\n",
    "\n",
    "Input sources:\n",
    "\n",
    "- [Apache Kafka](https://kafka.apache.org/)\n",
    "- Files (by continuously reading files from a directory\n",
    "- Sockets\n",
    "\n",
    "Sinks (data destination):\n",
    "\n",
    "- Apache Kafka\n",
    "- Files\n",
    "- Other computations\n",
    "- Memory (for debugging and testing)\n",
    "\n",
    "Still under development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdV_vbTENWu2"
   },
   "source": [
    "### Example: Process files in the  $DRIVE_DATA/by-day/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azWfdEFP4eV6"
   },
   "outputs": [],
   "source": [
    "!ls \"$DRIVE_DATA/by-day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVA5zSa9Nd8O"
   },
   "outputs": [],
   "source": [
    "# Check the format of a file\n",
    "!head \"$DRIVE_DATA\"/by-day/2010-12-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-u70JjDNf83"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame containing the data from one of the files\n",
    "dfStatic = spark.read\\\n",
    "                  .format(\"csv\")\\\n",
    "                  .option(\"header\", \"true\")\\\n",
    "                  .option(\"inferSchema\", \"true\")\\\n",
    "                  .load(os.environ[\"DRIVE_DATA\"]+\"/by-day/2010-12-01.csv\")\n",
    "dfStatic.printSchema()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPZKUFTBNh3F"
   },
   "outputs": [],
   "source": [
    "# Obtain a DataFrame containing the purchases per hour and per client throughout that day\n",
    "from pyspark.sql.functions import window, col, desc\n",
    "purchasePerClientPerHourStatic =\\\n",
    "             dfStatic.select(\n",
    "                                col(\"CustomerId\"), \n",
    "                                (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"), \n",
    "                                col(\"InvoiceDate\"))\\\n",
    "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "                       .sum(\"total_cost\")\n",
    "                            \n",
    "purchasePerClientPerHourStatic.show(15, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdANptAtNkSg"
   },
   "outputs": [],
   "source": [
    "# Because of the shuffling, the number of partitions is quite large\n",
    "print(purchasePerClientPerHourStatic.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYaCT4LbNmpM"
   },
   "outputs": [],
   "source": [
    "# Change the number of partitions to use and create the DataFrame again\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "purchasePerClientPerHourStatic =\\\n",
    "                            dfStatic.select(\n",
    "                                col(\"CustomerId\"), \n",
    "                                (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"), \n",
    "                                col(\"InvoiceDate\"))\\\n",
    "                            .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "                            .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0hJRMM1NnTo"
   },
   "outputs": [],
   "source": [
    "print(purchasePerClientPerHourStatic.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw4fanywNq0K"
   },
   "outputs": [],
   "source": [
    "# Define a DataFrame in Streaming that takes as a data source\n",
    "# the files in the $DRIVE_DATA/by-day/ directory.\n",
    "# Set it to read 1 file each time it is triggered\n",
    "dfStreaming = spark.readStream\\\n",
    "                   .schema(dfStatic.schema)\\\n",
    "                   .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                   .format(\"csv\")\\\n",
    "                   .option(\"header\", \"true\")\\\n",
    "                   .load(os.environ[\"DRIVE_DATA\"]+\"/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n1XQiAwNtWF"
   },
   "outputs": [],
   "source": [
    "# From the previous Streaming DataFrame, we obtain the purchases per hour and per client\n",
    "purchasePerClientPerHourStreaming = \\\n",
    "            dfStreaming.select(\n",
    "                               col(\"CustomerId\"), \n",
    "                              (col(\"UnitPrice\")*col(\"Quantity\")).alias(\"total_cost\"), \n",
    "                               col(\"InvoiceDate\"))\\\n",
    "                       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "                       .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bGdyO44Nvub"
   },
   "outputs": [],
   "source": [
    "# Create a DataStreamWriter object to write the values of the previous DataFrame\n",
    "# Values are written to a in-memory table\n",
    "# The writing mode is \"complete\": the whole output is overwritten \n",
    "# Data can be accessed using the purchases_per_hour table\n",
    "# Data from the input are read every second\n",
    "lookupPurchases = purchasePerClientPerHourStreaming\\\n",
    "                    .writeStream\\\n",
    "                    .format(\"memory\")\\\n",
    "                    .queryName(\"purchases_per_hour\")\\\n",
    "                    .outputMode(\"complete\")\\\n",
    "                    .trigger(processingTime='1 seconds')\n",
    "print(type(lookupPurchases))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfbasRomNy3y"
   },
   "outputs": [],
   "source": [
    "# Methods defined for a DataStreamWriter\n",
    "[method_name for method_name in dir(lookupPurchases)\n",
    " if callable(getattr(lookupPurchases, method_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TijNjfeN012"
   },
   "outputs": [],
   "source": [
    "# Start access to the input data\n",
    "lookupPurchases.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRt0TIbmN3EF"
   },
   "outputs": [],
   "source": [
    "# Start showing each second the content of the table \n",
    "from time import sleep\n",
    "for x in range(20):\n",
    "    spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM purchases_per_hour\n",
    "            ORDER BY `sum(total_cost)` DESC\n",
    "            \"\"\").show(15, truncate=False)\n",
    "    sleep(1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
