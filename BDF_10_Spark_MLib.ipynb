{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_10_Spark_MLib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n8JD51WVauRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 15:07:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/26 15:07:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/12/26 15:07:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 10 - Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vCoMnv84_Hc"
   },
   "source": [
    "Library of ML parallel algorithms for massive data\n",
    "\n",
    "-   Machine learning classic algorithms: classification, regression, clustering, collaborative filtering\n",
    "-   Other algorithms: feature extraction, transformation, dimensionality reduction, and selection\n",
    "-   Tools to build, evaluate and adjust ML pipelines\n",
    "-   Other tools: linear algebra, statistics, data processing, etc.\n",
    "\n",
    "\n",
    "Two packages:\n",
    "\n",
    "-   **spark.mllib:** Original RDD-based API\n",
    "-   **spark.ml:** High-level API, based on DataFrames\n",
    "\n",
    "Documentation and APIS:\n",
    "\n",
    "- ML\n",
    "    - Guia: http://spark.apache.org/docs/latest/ml-guide.html\n",
    "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
    "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package\n",
    "- MLlib\n",
    "    - Guia: http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html\n",
    "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.package\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWMYgbMB5mLS"
   },
   "source": [
    "## Example\n",
    "\n",
    "Use the [KMeans](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) clustering algorithm to group data from vectors spread over two clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fg-_EowF52_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SparseVector(3, {1: 1.2}), SparseVector(3, {1: 1.1}), SparseVector(3, {0: 0.9, 2: 1.0}), SparseVector(3, {0: 1.0, 2: 1.1})]\n",
      "[0.  1.2 0. ]\n",
      "[0.  1.1 0. ]\n",
      "[0.9 0.  1. ]\n",
      "[1.  0.  1.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#  Define an array of 4 sparse vectors, 3 elements each \n",
    "sparseData = [\n",
    "     Vectors.sparse(3, {1: 1.2}),\n",
    "     Vectors.sparse(3, {1: 1.1}),\n",
    "     Vectors.sparse(3, {0: 0.9, 2: 1.0}),\n",
    "     Vectors.sparse(3, {0: 1.0, 2: 1.1})\n",
    " ]\n",
    "print(sparseData)\n",
    "for i in range(4):\n",
    "    print(sparseData[i].toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0FIwTWDw6Mzx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|row|           features|\n",
      "+---+-------------------+\n",
      "|  1|      (3,[1],[1.2])|\n",
      "|  2|      (3,[1],[1.1])|\n",
      "|  3|(3,[0,2],[0.9,1.0])|\n",
      "|  4|(3,[0,2],[1.0,1.1])|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Turn the array into a DataFrame\n",
    "dfSD = sc.parallelize([\n",
    "  (1, sparseData[0]),\n",
    "  (2, sparseData[1]),\n",
    "  (3, sparseData[2]),\n",
    "  (4, sparseData[3])\n",
    "]).toDF([\"row\", \"features\"])\n",
    "\n",
    "dfSD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6K9_4SXD7AqG"
   },
   "outputs": [],
   "source": [
    "# Create a KMeans model without training, with 2 clusters\n",
    "# For more information, see https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#module-pyspark.ml.clustering\n",
    "kmeans = KMeans()\\\n",
    "    .setInitMode(\"k-means||\")\\\n",
    "    .setFeaturesCol(\"features\")\\\n",
    "    .setPredictionCol(\"prediction\")\\\n",
    "    .setK(2)\\\n",
    "    .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.1\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 1.8.0_342\n",
      "Branch HEAD\n",
      "Compiled by user yumwang on 2022-10-15T09:47:01Z\n",
      "Revision fbbcf9434ac070dd4ced4fb9efe32899c6db12a9\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_5b0235cd5e70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tpfUDkqA9tju"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters centres: [array([0.  , 1.15, 0.  ]), array([0.95, 0.  , 1.05])]\n"
     ]
    }
   ],
   "source": [
    "# Adjust the model to the previous DataFrame and show the cluster centres\n",
    "kmModel = kmeans.fit(dfSD)\n",
    "print(\"Clusters centres: {0}\".format(kmModel.clusterCenters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iyqXUOLk-B-X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "|row|           features|prediction|\n",
      "+---+-------------------+----------+\n",
      "|  1|      (3,[1],[1.2])|         0|\n",
      "|  2|      (3,[1],[1.1])|         0|\n",
      "|  3|(3,[0,2],[0.9,1.0])|         1|\n",
      "|  4|(3,[0,2],[1.0,1.1])|         1|\n",
      "+---+-------------------+----------+\n",
      "\n",
      "Cost = 0.014999999999999236\n"
     ]
    }
   ],
   "source": [
    "# Verify that the model clusters the data from the previous array\n",
    "kmModel.transform(dfSD).show()\n",
    "# Calculate the cost as the addition of the squared distance between the input points\n",
    "# and the centres of the corresponding clusters\n",
    "print(\"Cost = {0}\".format(\n",
    "    kmModel.summary.trainingCost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WG_qkJlC-ogG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+----------+\n",
      "|row|features                 |prediction|\n",
      "+---+-------------------------+----------+\n",
      "|1  |(3,[0,1,2],[0.9,1.0,1.0])|1         |\n",
      "|2  |(3,[1,2],[1.5,0.3])      |0         |\n",
      "+---+-------------------------+----------+\n",
      "\n",
      "Cost = 0.014999999999999236\n"
     ]
    }
   ],
   "source": [
    "# Test the model with other points\n",
    "dfTest = sc.parallelize([\n",
    "  (1, Vectors.sparse(3, {0: 0.9, 1:1.0, 2: 1.0})),\n",
    "  (2, Vectors.sparse(3, {1: 1.5, 2: 0.3}))\n",
    "]).toDF([\"row\", \"features\"])\n",
    "\n",
    "kmModel.transform(dfTest).show(truncate=False)\n",
    "\n",
    "# Calculate the cost as the addition of the squared distance between the input points\n",
    "# and the centres of the corresponding clusters\n",
    "print(\"Cost = {0}\".format(\n",
    "    kmModel.summary.trainingCost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wNwDiJtmBhtP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the model in a directory\n",
    "kmModel.save(\"/tmp/kmModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Y23ahGDqBolO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+----------+\n",
      "|row|features                 |prediction|\n",
      "+---+-------------------------+----------+\n",
      "|1  |(3,[0,1,2],[0.9,1.0,1.0])|1         |\n",
      "|2  |(3,[1,2],[1.5,0.3])      |0         |\n",
      "+---+-------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload the model\n",
    "sameModel = KMeansModel.load(\"/tmp/kmModel\")\n",
    "\n",
    "sameModel.transform(dfTest).show(truncate=False)\n",
    "# Calculate the cost as the addition of the squared distance between the input points\n",
    "# and the centres of the corresponding clusters\n",
    "#print(\"Cost = {0}\".format(sameModel.summary.trainingCost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PnuGHaLXB3fL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 58354)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /tmp/kmModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "09091546c7cda573c6363da0c63d2d5e80fd6f34e67ec03a9bc605494bb73032"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
