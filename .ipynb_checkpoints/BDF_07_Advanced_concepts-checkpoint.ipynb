{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_07_Advanced_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/spark': No such file or directory\n",
      "ln: failed to create symbolic link '/content/spark': No such file or directory\n",
      "/content/spark/\n",
      "DRIVE_DATA=/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n8JD51WVauRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 13:32:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 07 - Advanced concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMcqoRTlqa-X"
   },
   "source": [
    "We will show some additional concepts of Apache Spark\n",
    "\n",
    "- How a Spark application is executed\n",
    "- Use of broadcast variables and accummulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJmCZyf_rGQE"
   },
   "source": [
    "## Execution of a Spark application\n",
    "\n",
    "How the Spark code is executed\n",
    "\n",
    "  - Logic and physical level\n",
    "  - Jobs, stages and tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_l_1lTns58O"
   },
   "source": [
    "### Logic and physical plan\n",
    "\n",
    "From a user code, Spark generates a *logic plan*\n",
    "\n",
    "  -  A DAG with the operations to perform\n",
    "  -  It does not include information on the physical system on which it is going to be executed\n",
    "  -  The *Catalyst* optimiser generates an optimised, logic plan\n",
    "  \n",
    "From the optimised logic plan, a physical plan is created:\n",
    "\n",
    "  - It specifies how the logic plan will be executed in the cluster\n",
    "  - Different execution strategies wil be generated and compared using a cost model\n",
    "      - For example, how to perform a join in function of the characteristics of the data (size, partitions, etc.)\n",
    "\n",
    "The physical plan is executed in the cluster\n",
    "\n",
    "  - The execution is performed on RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utmB_4E9wrif"
   },
   "source": [
    "### Jobs, stages and tasks\n",
    "-   As seen, a Spark program defines a DAG connecting the different RDDs\n",
    "    -   *Transformations* create children RDDs from the parent RDDs\n",
    "\n",
    "-   *Actions* translate this DAG into an execution plan by generating a **Spark job**\n",
    "    -   The driver sends a *job* to compute all the RDDs involved in the action\n",
    "    -   A job comprises one or more *stages*\n",
    "    -   Each stage is associated to one or more RDDs from the DAG\n",
    "    -  Stages represent groups of *tasks* which run in parallel\n",
    "        - The stages are processed in order, launching individual tasks to compute segments of the RDDs\n",
    "        - Each task runs one or more transformations on a partition\n",
    "        - Tasks are executed in the cluster nodes\n",
    "    - A stage ends when a *shuffle* operation is performed\n",
    "        - it implies data movement among the cluster nodes\n",
    "\n",
    "\n",
    "-   Pipelining: several RDDs can be computed in the same stage if they verify that:\n",
    "    -   The RDDs can be obtained from their parents without data movement (e.g. *select*, *filter* or *map*), or if any of the RDDs has been cached on memory or disk\n",
    "    - The output of each operation is sent to the input of the following one without going down to disk\n",
    "\n",
    "- Shuffling persistence\n",
    "  -  Before a shuffling operation, data are written to a local disk\n",
    "  -  That allows re-launching failed tasks without the need to recompute all the previous transformations\n",
    "  -  Not performed is the data to shuffle have already been cached (using `cache` or `persist`)\n",
    "\n",
    "\n",
    "-   The *Spark web interface* shows information about the stages and tasks (more information: `toDebugString()` method in the RDDs)\n",
    "\n",
    "- The DataFrame's `explain` method, or RDD's `toDebugString` method shows the physical plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lffRje1M2LII"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(7) HashAggregate(keys=[], functions=[sum(id#8L)])\n",
      "   +- ShuffleQueryStage 4\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=264]\n",
      "         +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#8L)])\n",
      "            +- *(6) Project [id#8L]\n",
      "               +- *(6) SortMergeJoin [id#8L], [id#2L], Inner\n",
      "                  :- *(4) Sort [id#8L ASC NULLS FIRST], false, 0\n",
      "                  :  +- AQEShuffleRead coalesced\n",
      "                  :     +- ShuffleQueryStage 2\n",
      "                  :        +- Exchange hashpartitioning(id#8L, 200), ENSURE_REQUIREMENTS, [plan_id=135]\n",
      "                  :           +- *(3) Project [(id#0L * 5) AS id#8L]\n",
      "                  :              +- ShuffleQueryStage 0\n",
      "                  :                 +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=68]\n",
      "                  :                    +- *(1) Range (2, 10000000, step=2, splits=8)\n",
      "                  +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "                     +- AQEShuffleRead coalesced\n",
      "                        +- ShuffleQueryStage 3\n",
      "                           +- Exchange hashpartitioning(id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=80]\n",
      "                              +- ShuffleQueryStage 1\n",
      "                                 +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=76]\n",
      "                                    +- *(2) Range (2, 10000000, step=4, splits=8)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[sum(id#8L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=44]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#8L)])\n",
      "         +- Project [id#8L]\n",
      "            +- SortMergeJoin [id#8L], [id#2L], Inner\n",
      "               :- Sort [id#8L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#8L, 200), ENSURE_REQUIREMENTS, [plan_id=36]\n",
      "               :     +- Project [(id#0L * 5) AS id#8L]\n",
      "               :        +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=26]\n",
      "               :           +- Range (2, 10000000, step=2, splits=8)\n",
      "               +- Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=37]\n",
      "                     +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=29]\n",
      "                        +- Range (2, 10000000, step=4, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,col\n",
    "\n",
    "# Example to visualize the physical plan\n",
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.select(sum(col(\"id\")))\n",
    "\n",
    "step4.collect()\n",
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxaKass63KDM"
   },
   "source": [
    "### Broadcast variables\n",
    "\n",
    "-   By default, all shared variables (not RDDs) are sent to all executors\n",
    "\n",
    "    -   They are forwarded on each operation in which they appear\n",
    "\n",
    "-   Broadcast variables: Send, in an efficient way, read-only variables to the workers\n",
    "\n",
    "    -   They are sent only once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IBYFrR5j0jdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alpha', -3), ('beta', 3), ('gamma', 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "# dicc is a broadcast variable\n",
    "dicc=sc.broadcast({\"a\":\"alpha\",\"b\":\"beta\",\"c\":\"gamma\"})\n",
    "\n",
    "rdd=sc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\n",
    "\n",
    "# python 2\n",
    "#reduced_rdd = rdd.reduceByKey(add).map(lambda (x,y): (dicc.value[x],y))\n",
    "\n",
    "# python 3\n",
    "reduced_rdd = rdd.reduceByKey(add).map(lambda x: (dicc.value[x[0]],x[1]))\n",
    "\n",
    "print(reduced_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-sYM8TQ3VSq"
   },
   "source": [
    "### Accumulators\n",
    "\n",
    "Aggregate values from the *worker nodes*, which are then sent to the *driver*\n",
    "\n",
    "-   Useful to count events\n",
    "\n",
    "-   Only the driver can access its value\n",
    "\n",
    "-   Accumulators used on RDDs transformations could be incorrect\n",
    "\n",
    "    -   If the RDD is recalculated, the accumulator can be updated\n",
    "\n",
    "    -   This problem does not happen with actions\n",
    "\n",
    "-   By default, accumulators are integers or floats\n",
    "-  \"Custom accumulators\" can be created using [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6O6GvCBI3tka"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  5|\n",
      "|  9|\n",
      "|  5|\n",
      "|  9|\n",
      "| 10|\n",
      "|  9|\n",
      "|  8|\n",
      "|  9|\n",
      "|  4|\n",
      "| 10|\n",
      "|  2|\n",
      "|  4|\n",
      "|  2|\n",
      "|  5|\n",
      "|  9|\n",
      "|  8|\n",
      "|  6|\n",
      "|  3|\n",
      "|  1|\n",
      "|  9|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of even values: 5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from random import randint\n",
    "\n",
    "# Create a DataFrame from a list of Row objects\n",
    "# with random integers\n",
    "l = [Row(randint(1,10)) for n in range(10000)]\n",
    "df = spark.createDataFrame(l)\n",
    "df.show()\n",
    "# Define an accumulator\n",
    "neven = sc.accumulator(0)\n",
    "\n",
    "# if the number in a row is even, we increment the accumulator\n",
    "def isEven(row):\n",
    "    global neven\n",
    "    if row[\"_1\"]%2 == 0:\n",
    "        neven += 1\n",
    "print(neven)\n",
    "# Execute the function once per row\n",
    "df.foreach(isEven)\n",
    "\n",
    "print(\"Number of even values: {0}\".format(neven.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
