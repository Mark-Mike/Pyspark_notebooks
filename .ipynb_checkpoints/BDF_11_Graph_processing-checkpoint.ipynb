{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_11_Graph_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz   \n",
    "!tar xf spark-3.2.2-bin-hadoop2.7.tgz  \n",
    "!rm spark-3.2.2-bin-hadoop2.7.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libpsl5 publicsuffix\n",
      "The following NEW packages will be installed:\n",
      "  libpsl5 publicsuffix wget\n",
      "0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 1149 kB of archives.\n",
      "After this operation, 4001 kB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] "
     ]
    }
   ],
   "source": [
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-01 02:00:52--  https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, ::ffff:151.101.2.132, 2a04:4e42::644\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 272866820 (260M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.2.3-bin-hadoop2.7.tgz’\n",
      "\n",
      "spark-3.2.3-bin-had 100%[===================>] 260.23M  7.27MB/s    in 34s     \n",
      "\n",
      "2023-01-01 02:01:28 (7.71 MB/s) - ‘spark-3.2.3-bin-hadoop2.7.tgz’ saved [272866820/272866820]\n",
      "\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!wget https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.2.3-bin-hadoop2.7.tgz\n",
    "!rm spark-3.2.3-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./spark-3.2.3-bin-hadoop2.7 ./content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2W8ScIqhafBT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-01 02:02:47--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
      "Resolving repos.spark-packages.org (repos.spark-packages.org)... ::ffff:18.155.129.45, ::ffff:18.155.129.91, ::ffff:18.155.129.93, ...\n",
      "Connecting to repos.spark-packages.org (repos.spark-packages.org)|::ffff:18.155.129.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 247880 (242K) [binary/octet-stream]\n",
      "Saving to: ‘graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
      "\n",
      "graphframes-0.8.2-s 100%[===================>] 242.07K   527KB/s    in 0.5s    \n",
      "\n",
      "2023-01-01 02:02:49 (527 KB/s) - ‘graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.1-spark3.0-s_2.12/graphframes-0.8.1-spark3.0-s_2.12.jar\n",
    "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.2.2-bin-hadoop2.7 /content/spark\n",
    "\n",
    "#!mv graphframes-0.8.1-spark3.0-s_2.12.jar /content/spark/jars/\n",
    "!mv graphframes-0.8.2-spark3.2-s_2.12.jar /content/spark/jars/\n",
    "\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\"\n",
    "\n",
    "!ls -l /content/\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"./content/spark/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './content/spark': No such file or directory\n",
      "./content/spark/\n",
      "total 0\n",
      "drwxr-xr-x 1 501 1000 4096 Nov 14 18:28 spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"./content/spark/\"\n",
    "\n",
    "!rm -r ./content/spark\n",
    "#!ln -s ./content/spark-3.2.3-bin-hadoop2.7 ./content/spark\n",
    "!mv ./content/spark-3.2.3-bin-hadoop2.7 ./content/spark\n",
    "\n",
    "!mv graphframes-0.8.2-spark3.2-s_2.12.jar ./content/spark/jars/\n",
    "\n",
    "!export SPARK_HOME=./content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\"\n",
    "\n",
    "!ls -l ./content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n8JD51WVauRN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./content/spark//bin/load-spark-env.sh: line 68: ps: command not found\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/app/work/BDF/content/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/01 02:03:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version 3.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"./content/spark/\"\n",
    "\n",
    "!python -V\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.addPyFile('./content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./content/spark//bin/load-spark-env.sh: line 68: ps: command not found\n",
      "./content/spark//bin/load-spark-env.sh: line 68: ps: command not found\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/app/work/BDF/content/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.3\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.16\n",
      "Branch HEAD\n",
      "Compiled by user sunchao on 2022-11-14T17:58:57Z\n",
      "Revision b53c341e0fefbb33d115ab630369a18765b7763d\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"packages\",\"graphframes:graphframes-0.8.1-spark3.0-s_2.12\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 11 - Graph processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc96GG2FFzA5"
   },
   "source": [
    "## GraphX: Graph processing with RDDs\n",
    "\n",
    "Parallel graph programming using Spark\n",
    "\n",
    "- Main abstraction: [*Graph*](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph)\n",
    "    -   Directed multigraph with properties assigned to vertices and edges \n",
    "    -   It is an extension of the RDDs\n",
    "- It includes graph constructors, basic operators ( *reverse*, *subgraph*…) and graph algorithms ( *PageRank*, *Triangle Counting*…)\n",
    "- Only availabe on Scala.\n",
    "\n",
    "Documentation: [spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
    "\n",
    "API: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szq16u_cIKxA"
   },
   "source": [
    "## Graphs in GraphX\n",
    "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/grapxgraph.png\" alt=\"Grafo en GraphX\" style=\"width: 50px;\"/>\n",
    "(Source: M.S. Malak, R. East \"Spark GraphX in action\", Manning, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HkF-qEfIZrO"
   },
   "source": [
    "### Example of a simple graph\n",
    "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/simpsonsgraph.png\" alt=\"Grafo de los Simpson\" style=\"width: 600px;\"/>\n",
    "(Source: P. Zecević, M. Bonaći \"Spark in action\", Manning, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6bmE-QvGf2r"
   },
   "source": [
    "## GraphFrames: : Graph processing with DataFrames\n",
    "\n",
    "In Python we can use [*GraphFrames*](https://graphframes.github.io/graphframes/docs/_site/quick-start.html) which wraps GraphX algorithms under the DataFrames API, providing a Python interface.\n",
    "\n",
    "- Support for multiple languages is on the works\n",
    "    - For now,  available for Scala and Python \n",
    "- Not yet integrated on Spark\n",
    "    - Available as an external package (https://spark-packages.org/package/graphframes/graphframes)\n",
    "\n",
    "More information:\n",
    "- Project web: https://graphframes.github.io/graphframes/docs/_site/\n",
    "- Python API : https://graphframes.github.io/graphframes/docs/_site/api/python/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l07_p2RRIu_f"
   },
   "source": [
    "### Graphs using pyspark and GraphFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: scala: not found\n"
     ]
    }
   ],
   "source": [
    "!scala --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1enKNqmMIy1Z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o66.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     17\u001b[0m e \u001b[38;5;241m=\u001b[39m sqlContext\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     18\u001b[0m   (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfriend\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     19\u001b[0m   (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     20\u001b[0m   (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelationship\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Create a GraphFrame\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mgraphframes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraphFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Query: Get in-degree of each vertex.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m g\u001b[38;5;241m.\u001b[39minDegrees\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/tmp/spark-03050b28-8f27-4a22-85c7-ce5cce083794/userFiles-eb919098-ac9c-4f2d-b337-61896765b3d5/graphframes-0.8.2-spark3.2-s_2.12.jar/graphframes/graphframe.py:68\u001b[0m, in \u001b[0;36mGraphFrame.__init__\u001b[0;34m(self, v, e)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqlContext \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39msql_ctx\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sqlContext\u001b[38;5;241m.\u001b[39m_sc\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api \u001b[38;5;241m=\u001b[39m \u001b[43m_java_api\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mID \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api\u001b[38;5;241m.\u001b[39mID()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSRC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm_gf_api\u001b[38;5;241m.\u001b[39mSRC()\n",
      "File \u001b[0;32m/tmp/spark-03050b28-8f27-4a22-85c7-ce5cce083794/userFiles-eb919098-ac9c-4f2d-b337-61896765b3d5/graphframes-0.8.2-spark3.2-s_2.12.jar/graphframes/graphframe.py:41\u001b[0m, in \u001b[0;36m_java_api\u001b[0;34m(jsc)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_java_api\u001b[39m(jsc):\n\u001b[1;32m     40\u001b[0m     javaClassName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.graphframes.GraphFramePythonAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentThread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetContextClassLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjavaClassName\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;241m.\u001b[39mnewInstance()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o66.loadClass.\n: java.lang.ClassNotFoundException: org.graphframes.GraphFramePythonAPI\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# The following example shows how to create a GraphFrame, query it, and run the PageRank algorithm.\n",
    "# Source: https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#from graphframes import *\n",
    "import graphframes\n",
    "\n",
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "\n",
    "g = graphframes.GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "g.edges.filter(\"relationship = 'follow'\").count()\n",
    "\n",
    "# Run PageRank algorithm, and show results.\n",
    "#results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "#results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 811:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          pagerank|\n",
      "+---+------------------+\n",
      "|  c|1.8994109890559092|\n",
      "|  b|1.0905890109440908|\n",
      "|  a|              0.01|\n",
      "+---+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIZQECRHuiDw"
   },
   "source": [
    "#Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usL5LynSI6rE"
   },
   "source": [
    "## Exercise 11.1:\n",
    "\n",
    "A long time ago in a galaxy far, far away, the characters of the Star Wars franchise interacted with each other in an endless series of films. An ancient Jedi order, called the *Data Guardians of the Galaxy* (not affiliated to Marvel's homonym :) registered all those interactions and saved them on a digital file so that they could be studied by the forthcoming generations. This file was originally called (guess it) `sw.txt`, and you will find it in the `/data` directory. \n",
    "\n",
    "Using pySpark, perform the following operations and answer the following questions:\n",
    "\n",
    "1. Load the `$DRIVE_DATA/sw.txt` file. Take into account that it is a JSON file.\n",
    "2. Using this information, create a graph of interactions between the Star Wars characters.\n",
    "3. How many different characters are there?\n",
    "4. How many interactions are there?\n",
    "5. Who is the central character in Star Wars (the one who interacts in most scenes)? \n",
    "6. Who is the character with the highest 'rank' in Star Wars (use the PageRank algorithm)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fM--zQhLyAYM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./content/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/app/work/BDF/content/spark/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/05 18:38:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version 3.2.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"./content/spark\"\n",
    "\n",
    "!python -V\n",
    "\n",
    "os.environ[\"DRIVE_DATA\"] = \"./data/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('ex11').master(\"local[4]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('./content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
    "\n",
    "print(\"PySpark version {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "./data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DRIVE_DATA\"] = \"./data/\"\n",
    "!echo $SPARK_HOME\n",
    "!echo $DRIVE_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version 3.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/05 18:38:35 WARN SparkContext: The path ./content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName('ex11').master(\"local[4]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.addPyFile('./content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
    "\n",
    "print(\"PySpark version {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "swDF = spark.read.text(os.environ[\"DRIVE_DATA\"] + \"sw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('value', StringType(), True)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|value                       |\n",
      "+----------------------------+\n",
      "|{                           |\n",
      "|  \"nodes\": [                |\n",
      "|    {                       |\n",
      "|      \"name\": \"DARTH VADER\",|\n",
      "|      \"value\": 190,         |\n",
      "|      \"colour\": \"#000000\"   |\n",
      "|    },                      |\n",
      "|    {                       |\n",
      "|      \"name\": \"R2-D2\",      |\n",
      "|      \"value\": 171,         |\n",
      "|      \"colour\": \"#bde0f6\"   |\n",
      "|    },                      |\n",
      "|    {                       |\n",
      "|      \"name\": \"CHEWBACCA\",  |\n",
      "|      \"value\": 145,         |\n",
      "|      \"colour\": \"#A0522D\"   |\n",
      "|    },                      |\n",
      "|    {                       |\n",
      "|      \"name\": \"BB-8\",       |\n",
      "|      \"value\": 40,          |\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "num_words_per_line = swDF.select(F.expr(r\"regexp_extract_all(value, '(\\\"[a-zA-ZÀ-ÿ]+\\\": \\\\[)')\").alias(\"lines\")).withColumn(\"num_words\", F.lit(F.size(F.col(\"lines\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|       lines|num_words|\n",
      "+------------+---------+\n",
      "|          []|        0|\n",
      "|[\"nodes\": []|        1|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "|          []|        0|\n",
      "+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_words_per_line.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|lines       |sum(num_words)|\n",
      "+------------+--------------+\n",
      "|[]          |0             |\n",
      "|[\"nodes\": []|1             |\n",
      "|[\"links\": []|1             |\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_words_per_line.groupBy(\"lines\").sum(\"num_words\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "swJsonDF = spark.read.option(\"multiline\",\"true\").json(os.environ[\"DRIVE_DATA\"] + \"sw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- links: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- source: long (nullable = true)\n",
      " |    |    |-- target: long (nullable = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      " |-- nodes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- colour: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swJsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               links|               nodes|\n",
      "+--------------------+--------------------+\n",
      "|[{0, 1, 32}, {2, ...|[{#000000, DARTH ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swJsonDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swJsonDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|      links|               nodes|\n",
      "+-----------+--------------------+\n",
      "| {0, 1, 32}|{#000000, DARTH V...|\n",
      "|  {2, 0, 2}|{#bde0f6, R2-D2, ...|\n",
      "| {0, 20, 5}|{#A0522D, CHEWBAC...|\n",
      "| {0, 4, 22}| {#eb5d00, BB-8, 40}|\n",
      "|{0, 18, 41}|{#4f4fb1, QUI-GON...|\n",
      "| {0, 21, 2}|{#808080, NUTE GU...|\n",
      "|{0, 15, 12}|  {#808080, PK-4, 4}|\n",
      "| {0, 22, 2}| {#808080, TC-14, 5}|\n",
      "| {0, 23, 8}|{#48D1CC, OBI-WAN...|\n",
      "|{24, 0, 11}|{#808080, DOFINE, 4}|\n",
      "| {0, 26, 3}| {#808080, RUNE, 11}|\n",
      "| {0, 27, 2}|{#808080, TEY HOW...|\n",
      "| {0, 8, 47}|{#191970, EMPEROR...|\n",
      "| {0, 29, 1}|{#808080, CAPTAIN...|\n",
      "| {0, 30, 1}|{#808080, SIO BIB...|\n",
      "| {13, 0, 2}|{#9a9a00, JAR JAR...|\n",
      "| {0, 19, 4}|{#808080, TARPALS...|\n",
      "| {0, 32, 9}|{#808080, BOSS NA...|\n",
      "| {0, 33, 2}|{#DDA0DD, PADME, 75}|\n",
      "| {0, 34, 9}|{#808080, RIC OLI...|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType, MapType, DoubleType\n",
    "from pyspark.sql.functions import col,from_json\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#nodesJSON = swJsonDF.withColumn(\"jsonData\",from_json(col(\"nodes\"),schema)).select(\"jsonData.*\")\n",
    "#nodesJSON = swJsonDF.withColumn(\"jsonData\",from_json(col(\"nodes\"), MapType(StringType(), StringType(), StringType())))\n",
    "# to zip arrays links and nodes\n",
    "jsonData = swJsonDF.withColumn(\"jsonData\",F.explode(F.arrays_zip(\"Links\", \"Nodes\"))).select(\"jsonData.links\", \"jsonData.nodes\")\n",
    "jsonData.show()\n",
    "#jsonData.withColumn(\"jsonData\",F.lit(jsonData.nodes['name'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- links: struct (nullable = true)\n",
      " |    |-- source: long (nullable = true)\n",
      " |    |-- target: long (nullable = true)\n",
      " |    |-- value: long (nullable = true)\n",
      " |-- nodes: struct (nullable = true)\n",
      " |    |-- colour: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"colour\",StringType(),True),\n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"value\",LongType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsonData.withColumn(\"jsonData\",F.lit(jsonData.nodes.colour)).show()\n",
    "nodesDF = spark.createDataFrame(jsonData.select(jsonData.nodes['colour'], jsonData.nodes['name'], jsonData.nodes['value']).collect(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodesDF = nodesDF.na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "nodesDF = nodesDF.withColumn(\"id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----+---+\n",
      "| colour|          name|value| id|\n",
      "+-------+--------------+-----+---+\n",
      "|#000000|   DARTH VADER|  190|  0|\n",
      "|#bde0f6|         R2-D2|  171|  1|\n",
      "|#A0522D|     CHEWBACCA|  145|  2|\n",
      "|#eb5d00|          BB-8|   40|  3|\n",
      "|#4f4fb1|       QUI-GON|   62|  4|\n",
      "|#808080|   NUTE GUNRAY|   25|  5|\n",
      "|#808080|          PK-4|    4|  6|\n",
      "|#808080|         TC-14|    5|  7|\n",
      "|#48D1CC|       OBI-WAN|  148|  8|\n",
      "|#808080|        DOFINE|    4|  9|\n",
      "|#808080|          RUNE|   11| 10|\n",
      "|#808080|       TEY HOW|    5| 11|\n",
      "|#191970|       EMPEROR|   52| 12|\n",
      "|#808080|CAPTAIN PANAKA|   20| 13|\n",
      "|#808080|    SIO BIBBLE|    9| 14|\n",
      "|#9a9a00|       JAR JAR|   42| 15|\n",
      "|#808080|       TARPALS|    4| 16|\n",
      "|#808080|     BOSS NASS|    5| 17|\n",
      "|#DDA0DD|         PADME|   75| 18|\n",
      "|#808080|      RIC OLIE|   12| 19|\n",
      "+-------+--------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(colour='#808080', name='ELLO ASTY', value=5, id=108),\n",
       " Row(colour='#808080', name='JESS', value=5, id=109),\n",
       " Row(colour='#808080', name='NIV LEK', value=4, id=110)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodesDF.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"src\",LongType(),True),\n",
    "    StructField(\"dst\",LongType(),True), \n",
    "    StructField(\"value\",LongType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "linksDF = spark.createDataFrame(jsonData.select(jsonData.links['source'], jsonData.links['target'], jsonData.links['value']).collect(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "linksDF = linksDF.na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|src|dst|value|\n",
      "+---+---+-----+\n",
      "|  0|  1|   32|\n",
      "|  2|  0|    2|\n",
      "|  0| 20|    5|\n",
      "|  0|  4|   22|\n",
      "|  0| 18|   41|\n",
      "|  0| 21|    2|\n",
      "|  0| 15|   12|\n",
      "|  0| 22|    2|\n",
      "|  0| 23|    8|\n",
      "| 24|  0|   11|\n",
      "|  0| 26|    3|\n",
      "|  0| 27|    2|\n",
      "|  0|  8|   47|\n",
      "|  0| 29|    1|\n",
      "|  0| 30|    1|\n",
      "| 13|  0|    2|\n",
      "|  0| 19|    4|\n",
      "|  0| 32|    9|\n",
      "|  0| 33|    2|\n",
      "|  0| 34|    9|\n",
      "+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linksDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linksDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphframes\n",
    "\n",
    "graph = graphframes.GraphFrame(nodesDF, linksDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_characters = graph.vertices.count()\n",
    "num_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_interactions = graph.edges.count()\n",
    "num_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|src|count(src)|\n",
      "+---+----------+\n",
      "| 26|         6|\n",
      "| 29|         7|\n",
      "| 65|         1|\n",
      "|  0|        39|\n",
      "| 50|         2|\n",
      "| 39|         3|\n",
      "| 95|         7|\n",
      "| 68|         8|\n",
      "|  6|         1|\n",
      "| 72|        14|\n",
      "| 87|         5|\n",
      "|  9|         3|\n",
      "| 63|         4|\n",
      "| 51|         8|\n",
      "| 52|         3|\n",
      "| 17|         6|\n",
      "|  5|         9|\n",
      "|  1|         2|\n",
      "| 10|         1|\n",
      "| 48|        10|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_count = graph.edges.groupby('src').agg({'src':'count'})\n",
    "src_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 254:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|dst|count(dst)|\n",
      "+---+----------+\n",
      "| 26|         3|\n",
      "| 29|         7|\n",
      "| 19|         7|\n",
      "| 54|         6|\n",
      "|  0|        13|\n",
      "| 22|         1|\n",
      "|  7|         5|\n",
      "| 34|        17|\n",
      "| 50|         5|\n",
      "| 94|         5|\n",
      "| 57|         2|\n",
      "| 32|         7|\n",
      "| 43|         4|\n",
      "| 98|         9|\n",
      "| 71|         3|\n",
      "| 87|         6|\n",
      "| 72|        12|\n",
      "| 58|         1|\n",
      "| 27|         5|\n",
      "| 79|         4|\n",
      "+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dst_count = graph.edges.groupby('dst').agg({'dst':'count'})\n",
    "dst_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+----------+-----+\n",
      "|src|dst|count(src)|count(dst)|total|\n",
      "+---+---+----------+----------+-----+\n",
      "| 26| 26|         6|         3|    9|\n",
      "| 29| 29|         7|         7|   14|\n",
      "|  0|  0|        39|        13|   52|\n",
      "| 22| 22|         2|         1|    3|\n",
      "| 50| 50|         2|         5|    7|\n",
      "| 94| 94|         5|         5|   10|\n",
      "| 32| 32|         6|         7|   13|\n",
      "| 98| 98|         1|         9|   10|\n",
      "| 87| 87|         5|         6|   11|\n",
      "| 72| 72|        14|        12|   26|\n",
      "| 58| 58|         2|         1|    3|\n",
      "|103|103|         1|         4|    5|\n",
      "| 33| 33|         6|         3|    9|\n",
      "|  5|  5|         9|         7|   16|\n",
      "|  1|  1|         2|        20|   22|\n",
      "| 96| 96|         7|         7|   14|\n",
      "| 10| 10|         1|         3|    4|\n",
      "| 44| 44|         3|         1|    4|\n",
      "|  3|  3|        11|         1|   12|\n",
      "| 83| 83|         1|         1|    2|\n",
      "+---+---+----------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = src_count.join(dst_count, src_count.src == dst_count.dst, \"inner\").select(col('src'), col('dst'), col('count(src)'), col('count(dst)'), (col('count(src)')+col('count(dst)')).alias('total'))\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(max(total)=52)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.agg({'total':'max'}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|src|dst|total|\n",
      "+---+---+-----+\n",
      "|  0|  0|   52|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts.select('src', 'dst', 'total').where(col('total')==52).show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
