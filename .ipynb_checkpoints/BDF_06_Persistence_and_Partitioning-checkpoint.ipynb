{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_06_Persistence_and_Partitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0HHBxQa1Hc"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcWXhOxia5yZ"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsAfQ0CrgnWf",
    "outputId": "76e33dd9-0ccc-46f2-c370-3f8bf1d83f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-26 10:50:12--  http://apache.osuosl.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
      "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.233.100, 64.50.236.52, 140.211.166.134, ...\n",
      "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.233.100|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 300965906 (287M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.2.0-bin-hadoop3.2.tgz’\n",
      "\n",
      "spark-3.2.0-bin-had 100%[===================>] 287.02M  3.28MB/s    in 88s     \n",
      "\n",
      "2021-11-26 10:51:40 (3.27 MB/s) - ‘spark-3.2.0-bin-hadoop3.2.tgz’ saved [300965906/300965906]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urlhmQ_ra_ba"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiOoj3rUgnVx",
    "outputId": "557f0049-fc90-4b95-df79-eef15b71904a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/spark': No such file or directory\n",
      "/content/spark/\n",
      "DRIVE_DATA=/content/gdrive/My Drive/Enseignement/2021-2022/ING3/BDA/BigDataFrameworks/data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2URH7tCHbDqf"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8JD51WVauRN",
    "outputId": "432b7934-17c9-4dbb-8015-d4a0630d2c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 02:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/26 02:51:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ar81vEOHauP2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KBMAZitVauMT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jajoV8LDbTCe"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 06 - Persistence and Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94O191-heh5o"
   },
   "source": [
    "We will show here two important aspects of Apache Spark\n",
    "\n",
    "- `Persistence`: how to store DataFrames and RDDs in a way so that they do not need to be recalculated\n",
    "- `Partitioning`:  how to specify and change the partitions of a DataFrame or RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOH011r_e5iz"
   },
   "source": [
    "## Persistence\n",
    "\n",
    "Issues when reusing an RDD several times:\n",
    "\n",
    "-   Spark recalculates the RDD as well as its dependencies every time an action is executed\n",
    "-   Very costly (particularly in iterative problems)\n",
    "\n",
    "Solution\n",
    "\n",
    "-   Keep the RDD in memory and/or disk\n",
    "-   Use `cache()` or `persist()` methods\n",
    "\n",
    "### Persistence levels (as defined in [`pyspark.StorageLevel`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html#pyspark.StorageLevel) and [`org.apache.spark.storage.StorageLevel`](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel))\n",
    " Level                | Space used  | CPU time     | Memory/Disk   | Comments\n",
    " :------------------: | :------:    | :-----:      | :-------------: | ------------------\n",
    " MEMORY_ONLY          |   High      |   Low        |     Memory   | Stores the RDD in the JVM as a non-serialised Java object. If the RDD does not fit in memory, some partitions will not be cached in memory and will be recalculated on the fly every time they are required. Default level in Java and Scala.\n",
    " MEMORY_ONLY_SER      |   Low       |   High       |     Memory   | Stores the RDD as a serialised Java object (a *byte array* per partition). Default level in Python, using [`pickle`](http://docs.python.org/2/library/pickle.html).\n",
    " MEMORY_AND_DISK      |   High      |   Medium     |     Both     | Stores the RDD in the JVM as a non-serialised Java object. If the RDD does not fit in memory, the partitions that do not fit will be spilled to disk and read from there every time they are required.\n",
    " MEMORY_AND_DISK_SER  |   Low       |   High       |     Both     | Similar to MEMORY_AND_DISK but using serialised objects.\n",
    " DISK_ONLY            |   Low       |   High       |     Disk     | Stores the RDD partitions only on disk.\n",
    " OFF_HEAP             |   Low       |   High       |   Memory     | Stores the serialised RDD using *off-heap* memory (outside the JVM's heap) which can reduce the overhead of the garbage collector.\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "### Persistence levels\n",
    "\n",
    "-   In Scala and Java, the default level is MEMORY\\_ONLY\n",
    "\n",
    "-   In Python, data are always serialised (by default as *pickled* objects)\n",
    "\n",
    "    - MEMORY_ONLY and MEMORY_AND_DISK levels are equivalent to MEMORY_ONLY_SER and MEMORY_AND_DISK_SER\n",
    "    - When creating the SparkContext it is possible to request a serialisation [`marshal`](https://docs.python.org/2/library/marshal.html#module-marshal) \n",
    "    \n",
    "```python\n",
    "sc = SparkContext(master=\"local\", appName=\"My app\", serializer=pyspark.MarshalSerializer())\n",
    "```\n",
    "    \n",
    "### Fault tolerance\n",
    "\n",
    "-   If a node with stored data fails, the RDD is recomputed \n",
    "\n",
    "    -   Adding `_2` to the persistence level, 2 copies of the RDD are stored\n",
    "        \n",
    "### Cache management\n",
    "\n",
    "-   LRU algorithm to manage the cache memory\n",
    "\n",
    "    -   For *only memory* levels, the old RDDs are deleted and recalculated\n",
    "    -   For *memory and disk* levels, partitions that do not fit in memory are spilled to disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLZBQ1uQiIKV"
   },
   "source": [
    "### Persistence with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nw_w69siiPBT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: False\n",
      "Level without persistence: Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DRIVE_DATA\"] = \"./data/\"\n",
    "\n",
    "dfFlightsData = (spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(os.environ[\"DRIVE_DATA\"] + \"2015-summary.csv\"))\n",
    "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
    "print(\"Level without persistence: {0}\".format(dfFlightsData.storageLevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W-T_B6PFiyIf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: True\n",
      "Persistence level by default: Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "dfFlightsData.cache()\n",
    "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
    "print(\"Persistence level by default: {0}\".format(dfFlightsData.storageLevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H4O3d_EYjC9h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: True\n",
      "New persistence level: Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "# To chanche the persistence level, we need first to remove it from cache\n",
    "dfFlightsData.unpersist()\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "dfFlightsData.persist(StorageLevel.MEMORY_ONLY_2)\n",
    "print(\"Cached: {0}\".format(dfFlightsData.is_cached))\n",
    "print(\"New persistence level: {0}\".format(dfFlightsData.storageLevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tb8hwepbdys2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/26 02:52:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/12/26 02:52:17 WARN BlockManager: Block rdd_26_0 replicated to only 0 peer(s) instead of 1 peers\n",
      "+--------------------+\n",
      "|   DEST_COUNTRY_NAME|\n",
      "+--------------------+\n",
      "|       United States|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|               Egypt|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|          Costa Rica|\n",
      "|             Senegal|\n",
      "|             Moldova|\n",
      "|       United States|\n",
      "|       United States|\n",
      "|              Guyana|\n",
      "|               Malta|\n",
      "|            Anguilla|\n",
      "|             Bolivia|\n",
      "|       United States|\n",
      "|             Algeria|\n",
      "|Turks and Caicos ...|\n",
      "|       United States|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Cached: False\n"
     ]
    }
   ],
   "source": [
    "# Persistence is not inherited in transformations\n",
    "dfData2 = dfFlightsData.select(\"DEST_COUNTRY_NAME\")\n",
    "dfData2.show()\n",
    "print(\"Cached: {0}\".format(dfData2.is_cached))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xbon05TIeoFF"
   },
   "source": [
    "### Persistence with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BLkFa5lkfYNp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: False\n",
      "Level without persistence: Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1000), 10)\n",
    "print(\"Cached: {0}\".format(rdd.is_cached))\n",
    "print(\"Level without persistence: {0}\".format(rdd.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P5ihPiyKf_2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: True\n",
      "Default persistence level: Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "rdd.cache()\n",
    "print(\"Cached: {0}\".format(rdd.is_cached))\n",
    "print(\"Default persistence level: {0}\".format(rdd.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nJJV7LmHgFJg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: False\n",
      "Default persistence level: Serialized 1x Replicated\n",
      "Cached: True\n",
      "New persistence level: Disk Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "# Take rdd out of the cache memory\n",
    "rdd.unpersist() \n",
    "print(\"Cached: {0}\".format(rdd.is_cached))\n",
    "print(\"Default persistence level: {0}\".format(rdd.getStorageLevel()))\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "print(\"Cached: {0}\".format(rdd.is_cached))\n",
    "print(\"New persistence level: {0}\".format(rdd.getStorageLevel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PEDSqbjWgLdF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached: False\n"
     ]
    }
   ],
   "source": [
    "# Persistence is not inherited in transformations\n",
    "rdd2 = rdd.map(lambda x: x*x)\n",
    "print(\"Cached: {0}\".format(rdd2.is_cached))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLPVAnFoiOmr"
   },
   "source": [
    "### Checkpointing with RDDs\n",
    "RDDs can be checkpointed, forcing them to be stored on disk.\n",
    "\n",
    "- I is a *lazy* operation: data are not stored on disk until an Action is dispatched\n",
    "- Future references to those RDDs will load them from disk instead of recomputing them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "olKUeTmWkzuI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-summary.csv       cite75_99.txt.tar.bz2   italianPosts.csv.bz2\n",
      "CP\t\t       country_codes.txt       myscript.py\n",
      "apat63_99.txt\t       dfSE-partition.parquet  people.txt\n",
      "apat63_99.txt.tar.bz2  dfSE.json\t       quijote.txt\n",
      "books\t\t       dfSE.parquet\t       sw.txt\n",
      "by-day\t\t       dfSE2.parquet\t       syslog\n",
      "cite75_99.txt\t       italianPosts.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p \"$DRIVE_DATA\"/CP\n",
    "!ls \"$DRIVE_DATA\"\n",
    "#just in case...\n",
    "#rm -rf \"$DRIVE_DATA\"/CP/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RDICH0GDlcqc"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100000))\n",
    "spark.sparkContext.setCheckpointDir(os.environ[\"DRIVE_DATA\"] + \"CP\")\n",
    "rdd.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1vAehUbdlqjw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/CP/:\n",
      "total 0\n",
      "drwxr-xr-x 1 root root 4096 Dec 26 02:52 ad9c2208-0254-48f8-90ba-5427572bcaa2\n",
      "\n",
      "./data/CP/ad9c2208-0254-48f8-90ba-5427572bcaa2:\n",
      "total 0\n"
     ]
    }
   ],
   "source": [
    "!ls -lR \"$DRIVE_DATA\"CP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YXAQYrxCl3c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/CP/:\n",
      "total 0\n",
      "drwxr-xr-x 1 root root 4096 Dec 26 02:52 ad9c2208-0254-48f8-90ba-5427572bcaa2\n",
      "\n",
      "./data/CP/ad9c2208-0254-48f8-90ba-5427572bcaa2:\n",
      "total 0\n",
      "drwxr-xr-x 1 root root 4096 Dec 26 02:52 rdd-34\n",
      "\n",
      "./data/CP/ad9c2208-0254-48f8-90ba-5427572bcaa2/rdd-34:\n",
      "total 384\n",
      "-rw-r--r-- 1 root root 37646 Dec 26 02:52 part-00000\n",
      "-rw-r--r-- 1 root root 37902 Dec 26 02:52 part-00001\n",
      "-rw-r--r-- 1 root root 37902 Dec 26 02:52 part-00002\n",
      "-rw-r--r-- 1 root root 37902 Dec 26 02:52 part-00003\n",
      "-rw-r--r-- 1 root root 37902 Dec 26 02:52 part-00004\n",
      "-rw-r--r-- 1 root root 56830 Dec 26 02:52 part-00005\n",
      "-rw-r--r-- 1 root root 62902 Dec 26 02:52 part-00006\n",
      "-rw-r--r-- 1 root root 62902 Dec 26 02:52 part-00007\n"
     ]
    }
   ],
   "source": [
    "rdd.count()\n",
    "!ls -lR \"$DRIVE_DATA\"CP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c4WZkbtql4QE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './data/CP/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"$DRIVE_DATA\"CP/\n",
    "!ls -lR \"$DRIVE_DATA\"CP/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_MNCZhFSiNy"
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "The number of partitions is a function of the cluster size or the number of blocks of the HDFS file\n",
    "\n",
    "-   It can be adjusted when creating or operating on an RDD\n",
    "    \n",
    "    - RDDs offer a greater control on their partitioning \n",
    "\n",
    "-   For DataFrames it is possible to modify it once created.\n",
    "\n",
    "-   The parallelism of RDDs derived from other ones depends on their parent's. \n",
    "\n",
    "-   Useful properties:\n",
    "    -    `spark.default.parallelism` For RDDs, numbre of partitions by default returned by default by transformations like parallelize, join and reduceByKey\n",
    "        - Fixed value for a SparkContext\n",
    "        - The property `sc.defaultParallelism` indicates its value\n",
    "    -    `spark.sql.shuffle.partitions` For DataFrames, number of partitions to use when using data in *wide* transformations\n",
    "        - It can be modified using `spark.conf.set`\n",
    "\n",
    "- Useful functions:\n",
    "    -   `rdd.getNumPartitions()` returns the number of partitions of the RDD\n",
    "    -   `rdd.glom()` returns a new RDD joining the elements on each partition into a list\n",
    "\n",
    "    - `repartition(n)` returns a new DataFrame or RDD with exactly `n` partitions\n",
    "    - `coalesce(n)` optimised version of `repartition`, allows avoiding data movement\n",
    "        - But only if you are decreasing the number of partitions.\n",
    "    - `partitionBy(n,[partitionFunc])` Partitioning by key, using a partitioning function (by default, a hash of the key)\n",
    "        - Only for key/value RDDs\n",
    "        - Ensures that pairs with the same key go to the same partition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYZ_QWpDtIhr"
   },
   "source": [
    "### Partitions and RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WRlo8ChDtVqv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions by default for RDDs: 8\n",
      "RDD pairs = [(1, 1), (2, 4), (3, 9), (4, 16), (2, 4), (4, 16), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs partitioning: [[(1, 1), (2, 4), (3, 9)], [(4, 16), (2, 4), (4, 16), (1, 1)]]\n",
      "Number of pair partitions = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions by default for RDDs: {0}\"\n",
    "       .format(sc.defaultParallelism))\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 2, 4, 1], 2)\n",
    "pairs = rdd.map(lambda x: (x, x*x))\n",
    "\n",
    "print(\"RDD pairs = {0}\".format(pairs.collect()))\n",
    "print(\"Pairs partitioning: {0}\".format(pairs.glom().collect()))\n",
    "print(\"Number of pair partitions = {0}\".format(pairs.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "c82bElrJtYGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction keeping partitions: [[(2, 8), (4, 32)], [(1, 2), (3, 9)]]\n"
     ]
    }
   ],
   "source": [
    "# Reduction keeping the number of partitions\n",
    "from operator import add\n",
    "print(\"Reduction keeping partitions: {0}\".format(\n",
    "        pairs.reduceByKey(add).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "v17olWQHtawC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction with 3 partitions: [[(3, 9)], [(1, 2), (4, 32)], [(2, 8)]]\n"
     ]
    }
   ],
   "source": [
    "# Reduction modifying the number of partitions\n",
    "print(\"Reduction with 3 partitions: {0}\".format(\n",
    "       pairs.reduceByKey(add, 3).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iJiIKnrctdAM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs4 with 4 partitions: [[], [(4, 16), (2, 4), (4, 16), (1, 1)], [], [(1, 1), (2, 4), (3, 9)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repartitions example\n",
    "pairs4 = pairs.repartition(4)\n",
    "print(\"pairs4 with {0} partitions: {1}\".format(\n",
    "        pairs4.getNumPartitions(),\n",
    "        pairs4.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "np0Zbz9Tth7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs2 with 2 partitions: [[(4, 16), (2, 4), (4, 16), (1, 1)], [(1, 1), (2, 4), (3, 9)]]\n"
     ]
    }
   ],
   "source": [
    "# Coalesce example\n",
    "pairs2 = pairs4.coalesce(2)\n",
    "print(\"pairs2 with {0} partitions: {1}\".format(\n",
    "        pairs2.getNumPartitions(),\n",
    "        pairs2.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "D9iiePlttqbW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions by key (4 partitions): [[(4, 16), (4, 16)], [(1, 1), (1, 1)], [(2, 4), (2, 4)], [(3, 9)]]\n"
     ]
    }
   ],
   "source": [
    "# Partitioning by key\n",
    "pairs_key = pairs2.partitionBy(4)\n",
    "print(\"Partitions by key ({0} partitions): {1}\".format(\n",
    "        pairs_key.getNumPartitions(),\n",
    "        pairs_key.glom().collect())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Y_9ibNU4ttCy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition by key (2 partitions): [[(1, 1), (1, 1), (3, 9)], [(4, 16), (2, 4), (4, 16), (2, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# Using a partitioning function\n",
    "def partitionEvenOdd(key):\n",
    "    if key%2:\n",
    "        return 0  # Odd keys go to partition 0\n",
    "    else:\n",
    "        return 1  # Even keys go to partition 1\n",
    "        \n",
    "pairs_evenodd = pairs2.partitionBy(2, partitionEvenOdd)\n",
    "print(\"Partition by key ({0} partitions): {1}\".format(\n",
    "        pairs_evenodd.getNumPartitions(),\n",
    "        pairs_evenodd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ej832t8ZtvyJ"
   },
   "source": [
    "### Partitions and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LErImSLruBNf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  4|\n",
      "|  3|  9|\n",
      "|  4| 16|\n",
      "|  2|  4|\n",
      "|  4| 16|\n",
      "|  1|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the RDD to a DataFrame\n",
    "dfPairs = pairs.toDF()\n",
    "dfPairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "le5CphstuYbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions of the DataFrame: 2\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame inherits the number of partitions from the RDD\n",
    "print(\"Number of partitions of the DataFrame: {0}\"\n",
    "      .format(dfPairs.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Jc1ackRUuatC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after a narrow transformation: 2\n"
     ]
    }
   ],
   "source": [
    "# A narrow transformation keeps the number of partitions\n",
    "print(\"Number of partitions after a narrow transformation: {0}\"\n",
    "      .format(dfPairs.replace(1, 2).rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  2|  2|\n",
      "|  2|  4|\n",
      "|  3|  9|\n",
      "|  4| 16|\n",
      "|  2|  4|\n",
      "|  4| 16|\n",
      "|  2|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPairs.replace(1, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Zmsrd1e1ue3Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after a wide transformation: 1\n"
     ]
    }
   ],
   "source": [
    "# A wide transformation does not keep the number of partitions\n",
    "print(\"Number of partitions after a wide transformation: {0}\"\n",
    "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hsDOxbhuuhv6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions after a wide transformation: 1\n"
     ]
    }
   ],
   "source": [
    "# It is possibe to specify the number of partitions to use in the wide transformation\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 2)\n",
    "print(\"Number of partitions after a wide transformation: {0}\"\n",
    "      .format(dfPairs.sort(\"_1\").rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8AASwLXuqUf"
   },
   "source": [
    "## Working at partition level\n",
    "\n",
    "A `map`  operation is applied to each element of the RDD (or a `foreach` for each row of the DataFrame)\n",
    "\n",
    "-  It may imply redundant operations (f.ex. opening a connection to a DB)\n",
    "\n",
    "-  It may not be very efficient \n",
    "\n",
    "`map` and `foreach` can be called once per partition:\n",
    "\n",
    "-   Methods `mapPartitions()`, `mapPartitionsWithIndex()` and `foreachPartition()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vBKnTr8vu0nr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4], [5, 6], [7, 8, 9]]\n",
      "[[3, 2], [7, 2], [11, 2], [24, 3]]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9], 4)\n",
    "print(nums.glom().collect())\n",
    "\n",
    "def addAndCount(iterator):\n",
    "    addCount = [0,0]\n",
    "    for i in iterator:\n",
    "        addCount[0] += i\n",
    "        addCount[1] += 1\n",
    "    return addCount\n",
    "\n",
    "# Call the addAndCount function once per partition\n",
    "# The iterator includes the values of the partition\n",
    "print(nums.mapPartitions(addAndCount).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yRmwCVh-u3FX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Partition 0', [3, 2]], ['Partition 1', [7, 2]], ['Partition 2', [11, 2]], ['Partition 3', [24, 3]]]\n"
     ]
    }
   ],
   "source": [
    "def addAndCountIndex(index, iterator):\n",
    "    return \"Partition \"+str(index), addAndCount(iterator)\n",
    "\n",
    "# index is the number of partition\n",
    "print(nums.mapPartitionsWithIndex(addAndCountIndex).glom().collect())\n",
    "print(nums.mapPartitionsWithIndex(addAndCountIndex).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hM5gMbebu5Uc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86\n",
      "\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "def f(iterator):\n",
    "    tempfich, tempname = tempfile.mkstemp(dir=tempdir,text=True)\n",
    "    for x in iterator: \n",
    "        print(list(iterator)[0]) \n",
    "        os.write(tempfich, (str(x)+'\\t').encode())\n",
    "    os.close(tempfich)\n",
    "\n",
    "tempdir = \"/tmp/foreachPartition\"\n",
    "\n",
    "if not os.path.exists(tempdir):\n",
    "    os.mkdir(tempdir)\n",
    "    # For each partition of the RDD, a temporary file is created.\n",
    "    # The values of the partition are written to that file.\n",
    "    nums.foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "puOkWKIqV906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\n",
      "drwxr-xr-x 2 root root 4096 Nov 21 10:19 .\n",
      "drwxrwxrwt 1 root root 4096 Nov 21 10:19 ..\n",
      "-rw------- 1 root root    2 Nov 21 10:19 tmp9b70_vb2\n",
      "-rw------- 1 root root    2 Nov 21 10:19 tmpel0v4a21\n",
      "-rw------- 1 root root    2 Nov 21 10:19 tmppdcf9rh9\n",
      "-rw------- 1 root root    2 Nov 21 10:19 tmpqb489prs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54624)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.9/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!ls -al /tmp/foreachPartition"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
