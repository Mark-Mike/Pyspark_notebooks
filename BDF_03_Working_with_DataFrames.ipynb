{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebookColab/blob/master/BDF_03_Working_with_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-h_wDcNlH_K"
   },
   "source": [
    "#00 - Configuration of Apache Spark on Collaboratory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvD4HBMi0ohY"
   },
   "source": [
    "###Installing Java, Spark, and Findspark\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This code installs Apache Spark 2.4.4, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsAfQ0CrgnWf"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget  http://apache.osuosl.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz  \n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz  \n",
    "!rm spark-3.3.1-bin-hadoop3.tgz    \n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4Kjvk_h1AHl"
   },
   "source": [
    "### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiOoj3rUgnVx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
    "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2022-2023/ING3/HPDA/BigDataFrameworks/data/\"\n",
    "\n",
    "!rm /content/spark\n",
    "!ln -s /content/spark-3.3.1-bin-hadoop3 /content/spark\n",
    "!export SPARK_HOME=/content/spark\n",
    "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "!echo $SPARK_HOME\n",
    "!env |grep  \"DRIVE_DATA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwU28K5f1H3P"
   },
   "source": [
    "### Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n-4asPkCgnVB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/23 13:21:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "PySpark version 3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python -V\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Example: shows the PySpark version\n",
    "print(\"PySpark version {0}\".format(sc.version))\n",
    "\n",
    "# Example: parallelise an array and show the 2 first elements\n",
    "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pth1GUUrgnUW"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tfoycrngnSJ"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkKGBZRvEwZL"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 03 - Working with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcr9KTJbJI_4"
   },
   "source": [
    "## Introduction to DataFrames\n",
    "We will see:\n",
    "\n",
    "  - How to create a DataFrame\n",
    "  - Basic operations on DataFrames\n",
    "      - Show rows\n",
    "      - Select columns\n",
    "      - Rename, add and delete columns\n",
    "      - Delete null values and duplicated rows\n",
    "      - Replace values\n",
    "  - Save DataFrames in different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu6TkZeNd5hz"
   },
   "source": [
    "## Creating DataFrames\n",
    "A DataFrame can be created in different ways:\n",
    "\n",
    "  - From a data sequence\n",
    "  - From Row-type objects \n",
    "  - From an RDD or a DataSet\n",
    "  - Reading data from a file\n",
    "      - Like in Hadoop, Spark supports different filesystems: local, HDFS, Amazon S3\n",
    "          - By and large, it supports any data source that can be read with Hadoop\n",
    "      - Spark can access different types of files: plain text, CSV, JSON, [Parquet](https://parquet.apache.org/), [ORC](https://orc.apache.org/), Sequence, etc\n",
    "        -   It also supports compressed files\n",
    "  - Accessing relational databases or noSQL databases\n",
    "    -   MySQL, Postgres, etc. using JDBC/ODBC\n",
    "    -  Hive, HBase, Cassandra, MongoDB, AWS Redshift, etc.\n",
    "    \n",
    "Some examples on how to create DataFrames below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDJ4UH8wfoO7"
   },
   "source": [
    "### From a sequence or a list of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/24 13:13:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# We create a SparkSession object (or we retrieve it if it is already created)\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"My application\") \\\n",
    ".config(\"spark.some.config.option\", \"some-value\") \\\n",
    ".master(\"local[4]\") \\\n",
    ".getOrCreate()\n",
    "# We get the SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uCnbq2bgf5Rd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "+---+---+---+\n",
      "|  n| n1| n2|\n",
      "+---+---+---+\n",
      "|  1|  2|  2|\n",
      "|  3|  4|  6|\n",
      "|  5|  6| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "# Creating a DataFrame from a range and adding two columns\n",
    "df = spark.range(1,7,2).toDF(\"n\") # from 1 increase by 2, until 7-1=6, 5+2>6 so we don't show it\n",
    "\n",
    "\n",
    "df.show()\n",
    "df.withColumn(\"n1\", col(\"n\")+1).withColumn(\"n2\", expr(\"2*n\")).show()\n",
    "# Note that in the call to 'expr' we can include SQL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JxcY86E-f9Ub"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  Name|mark|result|\n",
      "+------+----+------+\n",
      "|  Eric| 5.1|  Pass|\n",
      "|  John| 4.0|  Fail|\n",
      "|Manuel|null|  null|\n",
      "+------+----+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- mark: double (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DataFrame from a list of tuples\n",
    "l = [(\"Eric\", 5.1, \"Pass\"),\\\n",
    "     (\"John\", 4.0, \"Fail\"),\\\n",
    "     (\"Manuel\", None, None)]\n",
    "dfMarks = spark.createDataFrame(l, schema=[\"Name\", \"mark\", \"result\"])\n",
    "dfMarks.show()\n",
    "dfMarks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH7LS_dYgCmH"
   },
   "source": [
    "### Creating DataFrames with a schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g88zsnXygSlG"
   },
   "source": [
    "When creating a DataFrame, it is a good idea to specify its schema:\n",
    "\n",
    "  - The schema defines the names and data types of each column\n",
    "  - It uses an object of type ``StructType`` to define the name and type of the columns \n",
    "  - The data types used by Spark are defined in:\n",
    "      - For PySpark: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\n",
    "      - For Scala: https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.sql.types.package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uStMKrWmgcfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|  Name|mark|result|\n",
      "+------+----+------+\n",
      "|  Eric| 5.1|  Pass|\n",
      "|  John| 4.0|  Fail|\n",
      "|Manuel|null|  null|\n",
      "+------+----+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = false)\n",
      " |-- mark: float (nullable = true)\n",
      " |-- result: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, FloatType, StringType\n",
    "from pyspark.sql import Row\n",
    "# Define the DataFrame schema\n",
    "schemaMarks = StructType([\n",
    "    StructField(\"Name\", StringType(), False), # can be null\n",
    "    StructField(\"mark\", FloatType(), True),\n",
    "    StructField(\"result\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "# Create the DataFrame from a list of Row objects\n",
    "rows = [Row(\"Eric\", 5.1, \"Pass\"),\\\n",
    "         Row(\"John\", 4.0, \"Fail\"),\\\n",
    "         Row(\"Manuel\", None, None)]\n",
    "\n",
    "dfMarks = spark.createDataFrame(rows, schema=schemaMarks)\n",
    "dfMarks.show()\n",
    "dfMarks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-C7Wg96gd5s"
   },
   "source": [
    "### Creating DataFrames from a text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9bRWavNgk0U"
   },
   "source": [
    "Each file line is stored as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4nxsEccsgqER"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|\n",
      "|                                                                           |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with           |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or       |\n",
      "|re-use it under the terms of the Project Gutenberg License included        |\n",
      "|with this eBook or online at www.gutenberg.net                             |\n",
      "|                                                                           |\n",
      "|                                                                           |\n",
      "|Title: Don Quijote                                                         |\n",
      "|                                                                           |\n",
      "|Author: Miguel de Cervantes Saavedra                                       |\n",
      "|                                                                           |\n",
      "|Posting Date: April 27, 2010 [EBook #2000]                                 |\n",
      "|Release Date: December, 1999                                               |\n",
      "|                                                                           |\n",
      "|Language: Spanish                                                          |\n",
      "|                                                                           |\n",
      "|                                                                           |\n",
      "|*** START OF THIS PROJECT GUTENBERG EBOOK DON QUIJOTE ***                  |\n",
      "|                                                                           |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mount first the Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "import os\n",
    "os.environ[\"DRIVE_DATA\"] = \"./data/\"\n",
    "dfQuijote = spark.read.text(os.environ[\"DRIVE_DATA\"] + \"quijote.txt\")\n",
    "dfQuijote.show(truncate=False)  # set truncate to false to extend the table with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbijvVbcgtw3"
   },
   "source": [
    "### Creating DataFrames from a CSV file (revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDIe3EAEg4xo"
   },
   "source": [
    "As an example, we are going to use a file with questions and replies from Stack Exchange (https://stackexchange.com/) in Italian. \n",
    "It is a CVS file, with the following 13 fields:\n",
    "\n",
    "  0. ``nComs`` - Number of comments of the question of the reply\n",
    "  2. ``lastActivity`` - Date and hour of the last modification\n",
    "  3. ``userId`` - Owner's ID \n",
    "  4. ``body`` - Text of the question or reply\n",
    "  5. ``score`` - Score of the question or reply based on positive and negative votes\n",
    "  6. ``creationDate`` - Creation date and hour \n",
    "  6. ``numViewed`` - Number of times viewed (null if the question has never been viewed)\n",
    "  7. ``title`` - Question title (null if it is a reply)\n",
    "  8. ``tags`` - Tags assigned to the question (null if there are no tags assigned)\n",
    "  9. ``nAnswers`` - Number of replies related to the question (null if there are not any)\n",
    "  10. ``acceptedAnswerId`` - The ID of the accepted answer (null if the question has no accepted answer)\n",
    "  11. ``postType`` - Type of message: 1 question, 2 reply\n",
    "  12. ``id`` - Unique message identifier\n",
    "\n",
    "Fields are separated by the \"~\" symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H07ygUKhL0p"
   },
   "source": [
    "#### a) Read the file and infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V2Lzn1GMhXxz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfSEInferred = spark.read.format(\"csv\")\\\n",
    "                    .option(\"mode\", \"FAILFAST\")\\\n",
    "                    .option(\"sep\", \"~\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .option(\"nullValue\", \"null\")\\\n",
    "                    .option(\"compression\", \"bzip2\")\\\n",
    "                    .load(os.environ[\"DRIVE_DATA\"] +\"italianPosts.csv.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTjGjjNNhYUf"
   },
   "source": [
    "Some options:\n",
    "\n",
    "1. ``mode``: specifies what to do when it finds corrupted entries\n",
    "    - ``PERMISSIVE``: sets all fields to null when a corrupted entry is found (default value)\n",
    "    - ``DROPMALFORMED``: deletes the rows with corrupted entries \n",
    "    - ``FAILFAST``: returns an error when a corrupted entry is found\n",
    "2. ``sep``:  field delimiter (by default \",\")\n",
    "3. ``inferSchema``: whether column types must be inferred (by default \"false\")\n",
    "4. ``header``: if \"true\", the first line is taken as the header (by default \"false\")\n",
    "5. ``nullValue``: character or string thar represents a NULL in the file  (by default \"\")\n",
    "6. ``compression``: compression type (by default \"none\")\n",
    "  \n",
    "These options are similar for other types of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "V1atd4MqhghB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "|_c0|                 _c1|_c2|                 _c3|_c4|                 _c5| _c6|                 _c7|                 _c8| _c9|_c10|_c11|_c12|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "|  4|2013-11-11 18:21:...| 17|&lt;p&gt;The infi...| 23|2013-11-10 19:37:...|null|                null|                null|null|null|   2|1165|\n",
      "|  5|2013-11-10 20:31:...| 12|&lt;p&gt;Come cre...|  1|2013-11-10 19:44:...|  61|Cosa sapreste dir...| &lt;word-choice&gt;|   1|null|   1|1166|\n",
      "|  2|2013-11-10 20:31:...| 17|&lt;p&gt;Il verbo...|  5|2013-11-10 19:58:...|null|                null|                null|null|null|   2|1167|\n",
      "|  1|2014-07-25 13:15:...|154|&lt;p&gt;As part ...| 11|2013-11-10 22:03:...| 187|Ironic constructi...|&lt;english-compa...|   4|1170|   1|1168|\n",
      "|  0|2013-11-10 22:15:...| 70|&lt;p&gt;&lt;em&g...|  3|2013-11-10 22:15:...|null|                null|                null|null|null|   2|1169|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 5 rows\n",
    "dfSEInferred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "|_c0|                 _c1|_c2|                 _c3|_c4|                 _c5| _c6|                 _c7|                 _c8| _c9|_c10|_c11|_c12|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "|  4|2013-11-11 18:21:...| 17|&lt;p&gt;The infi...| 23|2013-11-10 19:37:...|null|                null|                null|null|null|   2|1165|\n",
      "|  5|2013-11-10 20:31:...| 12|&lt;p&gt;Come cre...|  1|2013-11-10 19:44:...|  61|Cosa sapreste dir...| &lt;word-choice&gt;|   1|null|   1|1166|\n",
      "|  2|2013-11-10 20:31:...| 17|&lt;p&gt;Il verbo...|  5|2013-11-10 19:58:...|null|                null|                null|null|null|   2|1167|\n",
      "|  1|2014-07-25 13:15:...|154|&lt;p&gt;As part ...| 11|2013-11-10 22:03:...| 187|Ironic constructi...|&lt;english-compa...|   4|1170|   1|1168|\n",
      "|  0|2013-11-10 22:15:...| 70|&lt;p&gt;&lt;em&g...|  3|2013-11-10 22:15:...|null|                null|                null|null|null|   2|1169|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+--------------------+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSEInferred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LjM7KCurhkRv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_c0', IntegerType(), True), StructField('_c1', TimestampType(), True), StructField('_c2', IntegerType(), True), StructField('_c3', StringType(), True), StructField('_c4', IntegerType(), True), StructField('_c5', TimestampType(), True), StructField('_c6', IntegerType(), True), StructField('_c7', StringType(), True), StructField('_c8', StringType(), True), StructField('_c9', IntegerType(), True), StructField('_c10', IntegerType(), True), StructField('_c11', IntegerType(), True), StructField('_c12', IntegerType(), True)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out how the schema was inferred\n",
    "dfSEInferred.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "r4Q9AQ4dhn5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: timestamp (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: timestamp (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way of getting the same result\n",
    "dfSEInferred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbK_qvf3hs5v"
   },
   "source": [
    "#### b) Read the file and specify the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g1xnLchfhzD6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nComs: int, lastActivity: timestamp, userId: bigint, body: string, score: int, creationDate: timestamp, numViewed: int, title: string, tags: string, nAnswers: int, acceptedAnswerId: bigint, postType: tinyint, id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "# We first create a list with each column header\n",
    "# Note: avoid spaces and non-ascii characters on column names\n",
    "header = ([\"nComs\", \"lastActivity\", \"userId\", \n",
    "            \"body\", \"score\", \"creationDate\", \"numViewed\", \"title\", \n",
    "            \"tags\", \"nAnswers\", \"acceptedAnswerId\", \"postType\", \"id\"])\n",
    "            \n",
    "# Define the schema for the elements of the table\n",
    "# StructType -> Defines a schema for the DF from a list of StructFields\n",
    "# StructField -> Defines the name and type of each column, and whether it is nullable or not (True field)\n",
    "dfSE_Schema = StructType([\n",
    "  StructField(header[0], IntegerType(), True),\n",
    "  StructField(header[1], TimestampType(), True),\n",
    "  StructField(header[2], LongType(), True),\n",
    "  StructField(header[3], StringType(), True),\n",
    "  StructField(header[4], IntegerType(), True),\n",
    "  StructField(header[5], TimestampType(), True),\n",
    "  StructField(header[6], IntegerType(), True),\n",
    "  StructField(header[7], StringType(), True),\n",
    "  StructField(header[8], StringType(), True),\n",
    "  StructField(header[9], IntegerType(), True),\n",
    "  StructField(header[10], LongType(), True),\n",
    "  StructField(header[11], ByteType(), True),\n",
    "  StructField(header[12], LongType(), True)\n",
    "  ])\n",
    "  \n",
    "dfSE = spark.read.format(\"csv\")\\\n",
    "                    .option(\"mode\", \"FAILFAST\")\\\n",
    "                    .option(\"sep\", \"~\")\\\n",
    "                    .option(\"inferSchema\", \"false\")\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .option(\"nullValue\", \"null\")\\\n",
    "                    .option(\"compression\", \"bzip2\")\\\n",
    "                    .schema(dfSE_Schema)\\\n",
    "                    .load(os.environ[\"DRIVE_DATA\"] +\"italianPosts.csv.bz2\")\n",
    "dfSE.cache()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "SwS3ZIcFh3y2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+---+\n",
      "|nComs|        lastActivity|userId|                body|score|        creationDate|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType| id|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+---+\n",
      "|    2|2013-11-09 09:20:...|     8|&lt;p&gt;What is ...|   20|2013-11-05 20:22:...|      204|What are the rule...|&lt;word-choice&g...|       4|               2|       1|  1|\n",
      "|    9|2013-11-05 20:44:...|    17|&lt;p&gt;Translat...|   18|2013-11-05 20:36:...|     null|                null|                null|    null|            null|       2|  2|\n",
      "|    0|2013-11-05 20:37:...|     6|&lt;p&gt;[don't k...|    6|2013-11-05 20:37:...|     null|                null|                null|    null|            null|       2|  3|\n",
      "|    6|2013-11-07 17:27:...|    18|&lt;p&gt;Si sotto...|    4|2013-11-05 20:38:...|     null|                null|                null|    null|            null|       2|  4|\n",
      "|    5|2013-11-06 01:00:...|    18|&lt;p&gt;I am int...|   13|2013-11-05 20:46:...|      130|Tanto va la gatta...|    &lt;proverbs&gt;|       1|            null|       1|  5|\n",
      "|    0|2013-11-06 14:56:...|    18|&lt;p&gt;When an ...|   10|2013-11-05 21:01:...|      165|How do English wo...|     &lt;grammar&gt;|       4|               8|       1|  6|\n",
      "|    6|2013-11-05 21:09:...|     6|&lt;p&gt;While th...|   24|2013-11-05 21:09:...|     null|                null|                null|    null|            null|       2|  8|\n",
      "|    0|2013-11-06 10:25:...|    22|&lt;p&gt;Personal...|    6|2013-11-05 21:15:...|     null|                null|                null|    null|            null|       2|  9|\n",
      "|    2|2014-02-02 13:21:...|    17|&lt;p&gt;I often ...|   10|2013-11-05 21:29:...|      134|Is it correct to ...|&lt;foreign-words...|       3|              12|       1| 10|\n",
      "|    0|2014-07-26 19:17:...|    12|&lt;p&gt;What's t...|    6|2013-11-05 21:30:...|       92|'aver sceso le sc...|&lt;grammar&gt;&l...|       1|            null|       1| 11|\n",
      "|    4|2013-11-05 21:36:...|    22|&lt;p&gt;Grammati...|   19|2013-11-05 21:36:...|     null|                null|                null|    null|            null|       2| 12|\n",
      "|    2|2013-11-05 21:56:...|    15|&lt;p&gt;''avere'...|    7|2013-11-05 21:40:...|     null|                null|                null|    null|            null|       2| 13|\n",
      "|    0|2013-11-07 17:27:...|    10|&lt;p&gt;I wasn't...|    1|2013-11-05 21:52:...|     null|                null|                null|    null|            null|       2| 14|\n",
      "|    1|2014-03-07 12:47:...|    37|&lt;p&gt;Are ther...|   13|2013-11-05 21:57:...|      557|Are there rules f...|    &lt;pronouns&gt;|       1|              20|       1| 15|\n",
      "|    0|2014-02-02 13:21:...|     8|&lt;p&gt;It's not...|    6|2013-11-05 22:00:...|     null|                null|                null|    null|            null|       2| 16|\n",
      "|    1|2013-11-06 09:32:...|    12|&lt;p&gt;What is ...|    4|2013-11-05 22:17:...|       76|'apparire' vs. 's...| &lt;word-choice&gt;|       3|            null|       1| 17|\n",
      "|    0|2013-11-06 09:53:...|    12|&lt;p&gt;Which on...|    2|2013-11-05 22:34:...|       78|&quot;l'FBI&quot;...|&lt;articles&gt;&...|       2|            null|       1| 18|\n",
      "|    0|2013-11-06 09:33:...|    12|&lt;p&gt;How does...|    2|2013-11-05 22:38:...|       69|&quot;con 'degli'...| &lt;word-choice&gt;|       2|              25|       1| 19|\n",
      "|    0|2013-11-05 23:07:...|     8|&lt;p&gt;From the...|   12|2013-11-05 22:42:...|     null|                null|                null|    null|            null|       2| 20|\n",
      "|    2|2013-11-05 22:47:...|    41|&lt;p&gt;IMO ther...|    2|2013-11-05 22:47:...|     null|                null|                null|    null|            null|       2| 21|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfSE.sort(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Y1wlNqCsh4Wv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nComs: integer (nullable = true)\n",
      " |-- lastActivity: timestamp (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- creationDate: timestamp (nullable = true)\n",
      " |-- numViewed: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- nAnswers: integer (nullable = true)\n",
      " |-- acceptedAnswerId: long (nullable = true)\n",
      " |-- postType: byte (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7JqL8PojOEa"
   },
   "source": [
    "## Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx69XYmUjUP7"
   },
   "source": [
    "### Show rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "oRtkoWUZjaHO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|nComs|        lastActivity|userId|                body|score|        creationDate|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|    4|2013-11-11 18:21:...|    17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null|                null|                null|    null|            null|       2|1165|\n",
      "|    5|2013-11-10 20:31:...|    12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61|Cosa sapreste dir...| &lt;word-choice&gt;|       1|            null|       1|1166|\n",
      "|    2|2013-11-10 20:31:...|    17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null|                null|                null|    null|            null|       2|1167|\n",
      "|    1|2014-07-25 13:15:...|   154|&lt;p&gt;As part ...|   11|2013-11-10 22:03:...|      187|Ironic constructi...|&lt;english-compa...|       4|            1170|       1|1168|\n",
      "|    0|2013-11-10 22:15:...|    70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|     null|                null|                null|    null|            null|       2|1169|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show(n) shows the first n rows (by default, n=20)\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "b4Nyk4RrjePd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------------+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+--------+----------------+--------+----+\n",
      "|nComs|lastActivity           |userId|body                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |score|creationDate           |numViewed|title                                                                                                 |tags                                                               |nAnswers|acceptedAnswerId|postType|id  |\n",
      "+-----+-----------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------------+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+--------+----------------+--------+----+\n",
      "|4    |2013-11-11 18:21:10.903|17    |&lt;p&gt;The infinitive tense is commonly used for expressing rules especially in signs (of any kind, not just road signs).&lt;/p&gt;&lt;p&gt;For instance&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Non fumare&lt;br&gt;  Non calpestare il prato&lt;br&gt;  Tenere la destra&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The language &quot;trick&quot; behind this use of the infinitive form is the omission of the clause &lt;em&gt;Si prega di&lt;/em&gt; or equivalent, so the above sentences are read as&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;&lt;em&gt;&lt;strong&gt;Si prega di&lt;/em&gt;&lt;/strong&gt; non fumare&lt;br&gt;  &lt;strong&gt;&lt;em&gt;Si prega di&lt;/em&gt;&lt;/strong&gt; non calpestare il prato&lt;br&gt;  &lt;strong&gt;&lt;em&gt;Si prega di&lt;/em&gt;&lt;/strong&gt; tenere la destra&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Such form is not used in everyday's spoken language, as it's a convention used for giving orders and stating rules in an impersonal and formal way.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;That being said, there's an official use of the infinitive tense as imperative, which is the negative imperative.&lt;/p&gt;&lt;p&gt;In Italian the positive imperative form goes as follows&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Tieni la destra!&lt;br&gt;  Parla con lei!&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;whereas the negative imperative is formed with &lt;strong&gt;&lt;em&gt;non + infinitive tense&lt;/em&gt;&lt;/strong&gt;, as in &lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Non tenere la destra!&lt;br&gt;  Non parlare con lei!&lt;/p&gt;&lt;/blockquote&gt;&lt;hr&gt;&lt;p&gt;As discussed in the comments, it's also nice to notice the differences and the similarities with other Romance languages, such as Spanish and French.&lt;/p&gt;&lt;p&gt;Apparently French has the same identical construct as Italian for expressing formal impersonal orders, for instance&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Ne pas fumer&lt;br&gt;  &lt;em&gt;Non fumare&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;which is again a shortening for&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Merci de ne pas fumer&lt;br&gt;  &lt;em&gt;Grazie di non fumare&lt;/em&gt; or more idiomatically &lt;em&gt;Si prega di non fumare&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;On the other hand Spanish behaves differently and it doesn't have a special construct for impersonal orders, rather just using the formal imperative form, which is formed with the subjunctive&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;No fume&lt;br&gt;  &lt;em&gt;Non fumare&lt;/em&gt;, but also &lt;em&gt;Non fumi&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Reduzca la velocidad&lt;br&gt;  &lt;em&gt;Ridurre la velocità&lt;/em&gt;, but also &lt;em&gt;Riduca la velocità&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;|23   |2013-11-10 19:37:54.187|null     |null                                                                                                  |null                                                               |null    |null            |2       |1165|\n",
      "|5    |2013-11-10 20:31:00.177|12    |&lt;p&gt;Come credo sia conosciuto da tutti quelli che usano viaggiare con l'automobile, molti italiani hanno uno strano rapporto con gli abbaglianti; alcuni li amano così tanto che preferiscono mantenerli sempre accesi, altri invece li usano per segnalare, se non addirittura per comunicare informazioni di vario genere, dalla presenza di autovelox alla protesta per presunte violazioni del codice della strada.&lt;/p&gt;&lt;p&gt;Al di lá delle considerazioni e dei commenti circa queste abitudini, mi piacerebbe sapere se il verbo &quot;sfanagliare&quot; è normalmente usato, e compreso, in tutte le regioni italiane o se, magari, ci sono altri verbi in uso, purchè simpatici come quello.&lt;/p&gt;&lt;p&gt;Laddove qualcuno non avesse compreso l'uso del aforementioned verbo, ecco un esempio:&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;&quot;Ehi!&quot; - dice il marito a sua moglie - &quot;Quello li mi sta sfanagliando, st***o!&quot;&lt;/p&gt;    &lt;p&gt;E la moglie, &quot;Caro, rallenta; magari più avanti c'è un autovelox, cribbio!&quot;&lt;/p&gt;&lt;/blockquote&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |1    |2013-11-10 19:44:53.797|61       |Cosa sapreste dirmi della diffusione del verbo &quot;sfanagliare&quot; nelle diverse regioni italiane?|&lt;word-choice&gt;                                                |1       |null            |1       |1166|\n",
      "|2    |2013-11-10 20:31:00.177|17    |&lt;p&gt;Il verbo &lt;strong&gt;&lt;em&gt;sfanagliare&lt;/em&gt;&lt;/strong&gt; è un verbo inventato, non esistente nella lingua italiana, ma questo penso fosse chiaro dalla domanda.&lt;/p&gt;&lt;p&gt;Personalmente non l'ho mai sentito usare nel nord Italia, quindi non credo abbia una diffusione regionale omogenea. Un'alternativa che mi è capitato invece di sentire  è &lt;strong&gt;&lt;em&gt;sfanalare&lt;/em&gt;&lt;/strong&gt;, con significato identico.&lt;/p&gt;&lt;p&gt;Ad ogni modo, non sono sicuro dell'interpretazione di &lt;strong&gt;&lt;em&gt;sfanalare&lt;/em&gt;&lt;/strong&gt;/&lt;em&gt;&lt;strong&gt;sfanagliare&lt;/em&gt;&lt;/strong&gt; con il significato di &lt;strong&gt;&lt;em&gt;accecare con gli abbaglianti&lt;/em&gt;&lt;/strong&gt;. Probabilmente il significato che gli attribuirei è lo stesso di &lt;strong&gt;&lt;em&gt;fare i fari&lt;/em&gt;&lt;/strong&gt;, ossia &lt;strong&gt;&lt;em&gt;segnalare qualcosa tramite i fari abbaglianti&lt;/em&gt;&lt;/strong&gt;, solitamente accendendoli e spegnedoli ripetutamente. Per esempio&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Non mi ero accorto che il semaforo fosse diventato verde e il tizio dietro mi ha fatto i fari&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;(ok l'esempio è un po' tirato, sappiamo benissimo che il tizio di turno normalmente suona il clacson e tira una bestemmia...)&lt;/p&gt;&lt;p&gt;oppure&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Una vecchia consuetudine italiana era quella di sfanalare per segnalare la presenza della polizia stradale&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Nell'esempio che hai menzionato userei invece qualcosa del tipo&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Quello lì mi sta abbagliando (con sti c***o di fari, &lt;em&gt;ndt&lt;/em&gt;), st***o!&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;or&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Quello lì mi sta accecando (con sti c***o di fari, &lt;em&gt;ndt&lt;/em&gt;), st***o!&lt;/p&gt;&lt;/blockquote&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |5    |2013-11-10 19:58:02.1  |null     |null                                                                                                  |null                                                               |null    |null            |2       |1167|\n",
      "|1    |2014-07-25 13:15:02.27 |154   |&lt;p&gt;As part of my masters in linguistics, I am taking a course on the subject of irony. We were given examples of sentences that are most likely ironic, as the English sentence &quot;he is not exceptionally smart&quot; (which has the structure &quot;he is not exceptionally X&quot;). This does not mean literally that he is smart at an exceptional level, but rather, ironically, that he is very stupid.&lt;/p&gt;&lt;p&gt;Are there similar constructions in Italian, preferably ones that involve superlative and negation?&lt;/p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |11   |2013-11-10 22:03:41.027|187      |Ironic constructions in Italian                                                                       |&lt;english-comparison&gt;&lt;translation&gt;&lt;phrase-request&gt;|4       |1170            |1       |1168|\n",
      "|0    |2013-11-10 22:15:17.693|70    |&lt;p&gt;&lt;em&gt;Non è furbissimo&lt;/em&gt; can be used in the same sense as your example; or &lt;em&gt;non è velocissimo&lt;/em&gt; for someone who's rather slow. Maybe adding &lt;em&gt;proprio&lt;/em&gt;: &lt;em&gt;non è proprio furbissimo&lt;/em&gt;, which is more explicit in denying the smartness of the person.&lt;/p&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |3    |2013-11-10 22:15:17.693|null     |null                                                                                                  |null                                                               |null    |null            |2       |1169|\n",
      "+-----+-----------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------------------+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+--------+----------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Say that we do not want to truncate the long fields\n",
    "dfSE.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "s1BBaaz-jgp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nComs=5, lastActivity=datetime.datetime(2013, 11, 10, 20, 31, 0, 177000), userId=12, body=\"&lt;p&gt;Come credo sia conosciuto da tutti quelli che usano viaggiare con l'automobile, molti italiani hanno uno strano rapporto con gli abbaglianti; alcuni li amano così tanto che preferiscono mantenerli sempre accesi, altri invece li usano per segnalare, se non addirittura per comunicare informazioni di vario genere, dalla presenza di autovelox alla protesta per presunte violazioni del codice della strada.&lt;/p&gt;&lt;p&gt;Al di lá delle considerazioni e dei commenti circa queste abitudini, mi piacerebbe sapere se il verbo &quot;sfanagliare&quot; è normalmente usato, e compreso, in tutte le regioni italiane o se, magari, ci sono altri verbi in uso, purchè simpatici come quello.&lt;/p&gt;&lt;p&gt;Laddove qualcuno non avesse compreso l'uso del aforementioned verbo, ecco un esempio:&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;&quot;Ehi!&quot; - dice il marito a sua moglie - &quot;Quello li mi sta sfanagliando, st***o!&quot;&lt;/p&gt;    &lt;p&gt;E la moglie, &quot;Caro, rallenta; magari più avanti c'è un autovelox, cribbio!&quot;&lt;/p&gt;&lt;/blockquote&gt;\", score=1, creationDate=datetime.datetime(2013, 11, 10, 19, 44, 53, 797000), numViewed=61, title='Cosa sapreste dirmi della diffusione del verbo &quot;sfanagliare&quot; nelle diverse regioni italiane?', tags='&lt;word-choice&gt;', nAnswers=1, acceptedAnswerId=None, postType=1, id=1166)\n",
      "\n",
      "\n",
      "Row(nComs=1, lastActivity=datetime.datetime(2014, 1, 16, 19, 56, 5, 933000), userId=63, body='&lt;p&gt;Suppose I want to translate an English sentence like &quot;I have walked in the park for a year.&quot; The first though I had was translating the sentence as follows.&lt;/p&gt;&lt;blockquote&gt;  &lt;p&gt;Ho camminato nel parco per un anno.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;It seems correct, except for the fact that Present Perfect is used to talk about a past event that is still relevant for the present. That means the sentence I used as example would be understood as saying that I am still walking in the park. Similarly, &quot;I have gone to that store since I was a teenager.&quot; would mean I am still going to that store.&lt;br&gt;That is not the meaning of &quot;ho camminato nel parco per un anno&quot; which means I am not walking anymore in the park.&lt;/p&gt;&lt;p&gt;I thought of using the Simple Past, but I am not sure how to use it with a time reference. Apart that, &lt;em&gt;camminavo nel parco&lt;/em&gt; still means I am not anymore walking.&lt;/p&gt;&lt;p&gt;How should I translate the Present Perfect used in English?&lt;/p&gt;', score=4, creationDate=datetime.datetime(2013, 11, 11, 11, 31, 2, 343000), numViewed=114, title='How should I translate the Present Perfect used in English?', tags='&lt;usage&gt;&lt;tenses&gt;&lt;english-comparison&gt;', nAnswers=2, acceptedAnswerId=1177, postType=1, id=1175)\n"
     ]
    }
   ],
   "source": [
    "# take(n) returns the first n rows as a Python list of Row objects\n",
    "list = dfSE.take(5)\n",
    "print(list[1])\n",
    "print(\"\\n\")\n",
    "# collect() returns the DataFrame as a Python list of Row objects\n",
    "# Warning: if the DataFrame is too large, it might collapse the Driver!\n",
    "list2 = dfSE.collect()\n",
    "print(list2[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hPDx7xvIjif4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|nComs|        lastActivity|userId|                body|score|        creationDate|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|    2|2013-11-10 20:31:...|    17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null|                null|                null|    null|            null|       2|1167|\n",
      "|    0|2013-11-11 10:58:...|    18|&lt;p&gt;Wow, wha...|    5|2013-11-11 10:58:...|     null|                null|                null|    null|            null|       2|1174|\n",
      "|    0|2013-11-11 18:20:...|   132|&lt;p&gt;I would ...|    8|2013-11-11 14:04:...|     null|                null|                null|    null|            null|       2|1181|\n",
      "|    0|2013-11-11 20:27:...|     8|&lt;p&gt;It's an ...|    4|2013-11-11 20:20:...|     null|                null|                null|    null|            null|       2|1191|\n",
      "|    0|2014-08-31 20:19:...|    12|&lt;p&gt;Giulia e...|    5|2013-11-12 12:04:...|      610|Cosa significa e ...|&lt;grammar&gt;&l...|       4|            null|       1|1211|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Original Number of rows = 1261; Number of sampled rows = 124\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# sample(withReplacement, fraction, seed=None) returns a new Dataframe with a fraction of the original rows\n",
    "dfSESampled = dfSE.sample(False, 0.1, seed=False)  # take 10% of data, withReplacement set to false, to avoid repetition because we take values randomly from the data set and we may repeat the data, True if we want to show repeated data.\n",
    "dfSESampled.show(5)\n",
    "print(\"Original Number of rows = {0}; Number of sampled rows = {1}\".format(dfSE.count(), dfSESampled.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "x-DOlABOjkc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled rows = 10\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|nComs|        lastActivity|userId|                body|score|        creationDate|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|    4|2013-11-11 18:21:...|    17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null|                null|                null|    null|            null|       2|1165|\n",
      "|    0|2013-11-10 22:15:...|    70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|     null|                null|                null|    null|            null|       2|1169|\n",
      "|    0|2013-11-12 11:24:...|    63|&lt;p&gt;Comparin...|    3|2013-11-11 12:58:...|       60|Using the conditi...|&lt;usage&gt;&lt;...|       2|            1180|       1|1179|\n",
      "|    5|2013-11-12 11:24:...|    19|&lt;p&gt;Unfortun...|    1|2013-11-12 11:02:...|     null|                null|                null|    null|            null|       2|1204|\n",
      "|    4|2013-11-12 13:29:...|    12|&lt;p&gt;Non so, ...|    1|2013-11-12 13:23:...|     null|                null|                null|    null|            null|       2|1219|\n",
      "|    4|2013-11-12 16:38:...|    19|&lt;p&gt;You can ...|    1|2013-11-12 16:03:...|     null|                null|                null|    null|            null|       2|1233|\n",
      "|    1|2013-11-13 19:16:...|    19|&lt;p&gt;Sicurame...|    4|2013-11-13 19:16:...|     null|                null|                null|    null|            null|       2|1255|\n",
      "|    0|2013-11-16 10:00:...|    37|&lt;p&gt;It is si...|    2|2013-11-16 09:47:...|     null|                null|                null|    null|            null|       2|1281|\n",
      "|    2|2013-11-16 15:55:...|    37|&lt;p&gt;Very sho...|   14|2013-11-16 15:22:...|     null|                null|                null|    null|            null|       2|1293|\n",
      "|    2|2013-11-17 15:50:...|    95|&lt;p&gt;I think ...|    5|2013-11-17 15:50:...|     null|                null|                null|    null|            null|       2|1299|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit(n) limits the number of rows calculated to n\n",
    "dfSE_10rows = dfSE.sample(False, 0.1, seed=None).limit(10)\n",
    "print(\"Number of sampled rows = {0}\".format(dfSE_10rows.count()))\n",
    "dfSE_10rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HXAspRdjnV-"
   },
   "source": [
    "### Execute an operation on each row\n",
    "The method `foreach` applies a function to each row\n",
    "\n",
    "- The DataFrame is not modified and no other DataFrames are created\n",
    "- `foreach` is executed in the Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "D3QC31oFjvjx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1165\n",
      "1169\n",
      "1179\n",
      "1204\n",
      "1219\n",
      "1233\n",
      "1255\n",
      "1281\n",
      "1293\n",
      "1299\n"
     ]
    }
   ],
   "source": [
    "def printid(f):\n",
    "    print(f[\"id\"])\n",
    "    \n",
    "# In theory, this code should print all values of the 'id' column.\n",
    "# Due to the way the notebook manages tasks, it is not possible to see any output.\n",
    "# Run it on a pyspark-shell to see the output.\n",
    "dfSE_10rows.foreach(printid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTZ2CtybjwQn"
   },
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GQamd1S0jznG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|1165|&lt;p&gt;The infi...|\n",
      "|1166|&lt;p&gt;Come cre...|\n",
      "|1167|&lt;p&gt;Il verbo...|\n",
      "|1168|&lt;p&gt;As part ...|\n",
      "|1169|&lt;p&gt;&lt;em&g...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The idBody object is of type <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Creates a new DataFrame by selecting columns by name\n",
    "dfIdBody = dfSE.select(\"id\", \"body\")\n",
    "dfIdBody.show(5)\n",
    "\n",
    "print(\"The idBody object is of type {0}\".format(type(dfIdBody)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zYlZy8j4j2sR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|1165|&lt;p&gt;The infi...|\n",
      "|1166|&lt;p&gt;Come cre...|\n",
      "|1167|&lt;p&gt;Il verbo...|\n",
      "|1168|&lt;p&gt;As part ...|\n",
      "|1169|&lt;p&gt;&lt;em&g...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way of specifying the columns to select\n",
    "dfIdBody2 = dfSE.select(dfSE.id, dfSE.body)\n",
    "dfIdBody2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RQWu1PaXj3aD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The colId object is of type <class 'pyspark.sql.column.Column'>\n",
      "The colCreateDate object is of type <class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# It is also possible to specify objects of Column type...\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "colId = col(\"id\")\n",
    "colCreateDate = col(\"creationDate\")\n",
    "print(\"The colId object is of type {0}\".format(type(colId)))\n",
    "print(\"The colCreateDate object is of type {0}\".format(type(colCreateDate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4SvgG2Sjj5BW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|  id|       Creation_date|             Content|\n",
      "+----+--------------------+--------------------+\n",
      "|1165|2013-11-10 19:37:...|&lt;p&gt;The infi...|\n",
      "|1166|2013-11-10 19:44:...|&lt;p&gt;Come cre...|\n",
      "|1167|2013-11-10 19:58:...|&lt;p&gt;Il verbo...|\n",
      "|1168|2013-11-10 22:03:...|&lt;p&gt;As part ...|\n",
      "|1169|2013-11-10 22:15:...|&lt;p&gt;&lt;em&g...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... and create a DataFrame from Column objects, by renaming the columns\n",
    "dfIdBodyDate = dfSE.select(colId, \n",
    "                              colCreateDate.alias(\"Creation_date\"), \n",
    "                              dfSE.body.alias(\"Content\"))\n",
    "dfIdBodyDate.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw45U9EYj9Nh"
   },
   "source": [
    "#### Select columns by using expressions\n",
    "\n",
    "To select columns using SQL expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jO43v_chkCg-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----------+\n",
      "|nComs|        lastActivity|userId|                body|score|        creationDate|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|ValidReply|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----------+\n",
      "|    4|2013-11-11 18:21:...|    17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null|                null|                null|    null|            null|       2|1165|     false|\n",
      "|    5|2013-11-10 20:31:...|    12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61|Cosa sapreste dir...| &lt;word-choice&gt;|       1|            null|       1|1166|      true|\n",
      "|    2|2013-11-10 20:31:...|    17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null|                null|                null|    null|            null|       2|1167|     false|\n",
      "|    1|2014-07-25 13:15:...|   154|&lt;p&gt;As part ...|   11|2013-11-10 22:03:...|      187|Ironic constructi...|&lt;english-compa...|       4|            1170|       1|1168|      true|\n",
      "|    0|2013-11-10 22:15:...|    70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|     null|                null|                null|    null|            null|       2|1169|     false|\n",
      "|    2|2013-11-10 22:17:...|    17|&lt;p&gt;There's ...|    8|2013-11-10 22:17:...|     null|                null|                null|    null|            null|       2|1170|     false|\n",
      "|    1|2013-11-11 09:51:...|    63|&lt;p&gt;As other...|    3|2013-11-11 09:51:...|     null|                null|                null|    null|            null|       2|1171|     false|\n",
      "|    1|2013-11-12 23:57:...|    63|&lt;p&gt;The expr...|    1|2013-11-11 10:09:...|     null|                null|                null|    null|            null|       2|1172|     false|\n",
      "|    9|2014-01-05 11:13:...|    63|&lt;p&gt;When I w...|    5|2013-11-11 10:28:...|      122|Is &quot;scancell...|&lt;usage&gt;&lt;...|       3|            1181|       1|1173|      true|\n",
      "|    0|2013-11-11 10:58:...|    18|&lt;p&gt;Wow, wha...|    5|2013-11-11 10:58:...|     null|                null|                null|    null|            null|       2|1174|     false|\n",
      "|    1|2014-01-16 19:56:...|    63|&lt;p&gt;Suppose ...|    4|2013-11-11 11:31:...|      114|How should I tran...|&lt;usage&gt;&lt;...|       2|            1177|       1|1175|      true|\n",
      "|    0|2013-11-11 14:36:...|    63|&lt;p&gt;Except w...|    3|2013-11-11 11:39:...|       58|Using a comma bet...|&lt;usage&gt;&lt;...|       2|            1182|       1|1176|      true|\n",
      "|    3|2014-01-16 19:56:...|    71|&lt;p&gt;Both you...|    6|2013-11-11 11:57:...|     null|                null|                null|    null|            null|       2|1177|     false|\n",
      "|    0|2013-11-11 12:00:...|    12|&lt;blockquote&gt...|    1|2013-11-11 12:00:...|     null|                null|                null|    null|            null|       2|1178|     false|\n",
      "|    0|2013-11-12 11:24:...|    63|&lt;p&gt;Comparin...|    3|2013-11-11 12:58:...|       60|Using the conditi...|&lt;usage&gt;&lt;...|       2|            1180|       1|1179|      true|\n",
      "|    4|2013-11-11 19:54:...|    18|&lt;p&gt;Using th...|    5|2013-11-11 13:48:...|     null|                null|                null|    null|            null|       2|1180|     false|\n",
      "|    0|2013-11-11 18:20:...|   132|&lt;p&gt;I would ...|    8|2013-11-11 14:04:...|     null|                null|                null|    null|            null|       2|1181|     false|\n",
      "|    1|2013-11-11 14:36:...|   132|&lt;p&gt;Putting ...|   11|2013-11-11 14:17:...|     null|                null|                null|    null|            null|       2|1182|     false|\n",
      "|    2|2013-11-14 09:56:...|    22|&lt;p&gt;Many peo...|    6|2013-11-11 14:43:...|      321|Can Dante Alighie...|&lt;history&gt;&l...|       1|            1263|       1|1183|      true|\n",
      "|    2|2013-11-11 23:23:...|   159|&lt;p&gt;Sono un'...|    7|2013-11-11 18:19:...|      138|origine dell'espr...|&lt;idioms&gt;&lt...|       2|            1185|       1|1184|      true|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use more complex expressions\n",
    "dfSE.selectExpr(\"*\", # Select all columns and set ValidReply to True for those with, at least, one reply.\n",
    "                \"(nAnswers IS NOT NULL) as ValidReply\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mSTG7pXokAd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|  ID|       Creation_date|             Content|\n",
      "+----+--------------------+--------------------+\n",
      "|1165|2013-11-10 19:37:...|&lt;p&gt;The infi...|\n",
      "|1166|2013-11-10 19:44:...|&lt;p&gt;Come cre...|\n",
      "|1167|2013-11-10 19:58:...|&lt;p&gt;Il verbo...|\n",
      "|1168|2013-11-10 22:03:...|&lt;p&gt;As part ...|\n",
      "|1169|2013-11-10 22:15:...|&lt;p&gt;&lt;em&g...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Same DataFrame as before but using expressions\n",
    "dfIdDateBodyExpr = dfSE.select(\n",
    "                           expr(\"id AS ID\"), \n",
    "                           expr('creationDate AS Creation_date'), \n",
    "                           expr(\"body AS Content\"))\n",
    "dfIdDateBodyExpr.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3fIFG0MkHY0"
   },
   "source": [
    "### Rename, add and delete columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "w4-CC8StkJ1r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------+-----+--------+\n",
      "|Creation_date          |Number_of_visits|score|postType|\n",
      "+-----------------------+----------------+-----+--------+\n",
      "|2013-11-10 19:37:54.187|null            |23   |2       |\n",
      "|2013-11-10 19:44:53.797|61              |1    |1       |\n",
      "|2013-11-10 19:58:02.1  |null            |5    |2       |\n",
      "|2013-11-10 22:03:41.027|187             |11   |1       |\n",
      "|2013-11-10 22:15:17.693|null            |3    |2       |\n",
      "|2013-11-10 22:17:22.38 |null            |8    |2       |\n",
      "|2013-11-11 09:51:11.22 |null            |3    |2       |\n",
      "|2013-11-11 10:09:05.117|null            |1    |2       |\n",
      "|2013-11-11 10:28:12.613|122             |5    |1       |\n",
      "|2013-11-11 10:58:02.62 |null            |5    |2       |\n",
      "|2013-11-11 11:31:02.343|114             |4    |1       |\n",
      "|2013-11-11 11:39:12.703|58              |3    |1       |\n",
      "|2013-11-11 11:57:03.723|null            |6    |2       |\n",
      "|2013-11-11 12:00:25.583|null            |1    |2       |\n",
      "|2013-11-11 12:58:38.137|60              |3    |1       |\n",
      "|2013-11-11 13:48:24.287|null            |5    |2       |\n",
      "|2013-11-11 14:04:00.753|null            |8    |2       |\n",
      "|2013-11-11 14:17:44.79 |null            |11   |2       |\n",
      "|2013-11-11 14:43:47.487|321             |6    |1       |\n",
      "|2013-11-11 18:19:12.253|138             |7    |1       |\n",
      "+-----------------------+----------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the creationDate column\n",
    "dfSE = dfSE.withColumnRenamed(\"creationDate\", \"Creation_date\")\n",
    "dfSE.cache()\n",
    "dfSE.select(\"Creation_date\", \n",
    "            dfSE.numViewed.alias(\"Number_of_visits\"), \n",
    "            \"score\", \n",
    "            \"postType\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iFWWZ4askPrV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----+\n",
      "|nComs|        lastActivity|userId|                body|score|       Creation_date|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|ones|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----+\n",
      "|    4|2013-11-11 18:21:...|    17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null|                null|                null|    null|            null|       2|1165|   1|\n",
      "|    5|2013-11-10 20:31:...|    12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61|Cosa sapreste dir...| &lt;word-choice&gt;|       1|            null|       1|1166|   1|\n",
      "|    2|2013-11-10 20:31:...|    17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null|                null|                null|    null|            null|       2|1167|   1|\n",
      "|    1|2014-07-25 13:15:...|   154|&lt;p&gt;As part ...|   11|2013-11-10 22:03:...|      187|Ironic constructi...|&lt;english-compa...|       4|            1170|       1|1168|   1|\n",
      "|    0|2013-11-10 22:15:...|    70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|     null|                null|                null|    null|            null|       2|1169|   1|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'ones' with all its values set to 1\n",
    "from pyspark.sql.functions import lit\n",
    "# lit transforms a literal in Python to Spark internal format\n",
    "# (in this example, IntegerType)\n",
    "dfSE = dfSE.withColumn(\"ones\", lit(1))  # we can specify values we want to insert with lit, we can do lit(dfSE.score * 2) ....\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5wWzrupfkRth"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nComs',\n",
       " 'lastActivity',\n",
       " 'userId',\n",
       " 'body',\n",
       " 'score',\n",
       " 'Creation_date',\n",
       " 'numViewed',\n",
       " 'title',\n",
       " 'tags',\n",
       " 'nAnswers',\n",
       " 'acceptedAnswerId',\n",
       " 'postType',\n",
       " 'id']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes a column using drop\n",
    "dfSE = dfSE.drop(col(\"ones\"))\n",
    "dfSE.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsxTogrdkWt0"
   },
   "source": [
    "### Delete null and duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0XT8uw47kY2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number or rows: 1261; number of non null rows: 222\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows that have null on any of their columns\n",
    "dfNoNulls = dfSE.dropna(\"any\")\n",
    "print(\"Initial number or rows: {0}; number of non null rows: {1}\"\n",
    "       .format(dfSE.count(), dfNoNulls.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "O0xh2BW5ka_A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with all columns set to null: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove rows that have null on all their columns\n",
    "dfNeitherNull = dfSE.dropna(\"all\")\n",
    "print(\"Number of rows with all columns set to null: {0}\"\n",
    "       .format(dfSE.count() - dfNeitherNull.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NYKhJBTQkdbR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated rows\n",
    "dfWithoutDuplicates = dfSE.dropDuplicates()\n",
    "print(\"Number of duplicated rows: {0}\"\n",
    "       .format(dfSE.count() - dfWithoutDuplicates.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-Fgz5rXhkf-C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove rows when a given column is duplicated\n",
    "dfWithoutDuplicatedUser = dfSE.dropDuplicates([\"userId\"])\n",
    "print(\"Number of unique users: {0}\"\n",
    "       .format(dfWithoutDuplicatedUser.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B8CSyfr1kjWe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with numViewed AND acceptedAnswerId not null: 222\n",
      "Number of rows with numViewed OR acceptedAnswerId not null: 374\n"
     ]
    }
   ],
   "source": [
    "# Other examples\n",
    "dfNoNullnumViewedAcceptedAnswerId = dfSE.dropna(\"any\", subset=[\"numViewed\", \"acceptedAnswerId\"]) # null in any of the columns, both shouldn't be null\n",
    "print(\"Number of rows with numViewed AND acceptedAnswerId not null: {0}\"\n",
    "       .format(dfNoNullnumViewedAcceptedAnswerId.count()))\n",
    "\n",
    "dfNoNullnumViewedAcceptedAnswerId = dfSE.dropna(\"all\", subset=[\"numViewed\", \"acceptedAnswerId\"]) # null in all columns, 1 of the columns shouldn't be null\n",
    "print(\"Number of rows with numViewed OR acceptedAnswerId not null: {0}\"\n",
    "       .format(dfNoNullnumViewedAcceptedAnswerId.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR8rAZGCkln-"
   },
   "source": [
    "### Replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jGtTf0FmkuXv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|nComs|        lastActivity|userId|                body|score|       Creation_date|numViewed|               title|                tags|nAnswers|acceptedAnswerId|postType|  id|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "|    4|2013-11-11 18:21:...|    17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|        0|                null|                null|       0|            null|       2|1165|\n",
      "|    5|2013-11-10 20:31:...|    12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61|Cosa sapreste dir...| &lt;word-choice&gt;|       1|            null|       1|1166|\n",
      "|    2|2013-11-10 20:31:...|    17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|        0|                null|                null|       0|            null|       2|1167|\n",
      "|    1|2014-07-25 13:15:...|   154|&lt;p&gt;As part ...|   11|2013-11-10 22:03:...|      187|Ironic constructi...|&lt;english-compa...|       4|            1170|       1|1168|\n",
      "|    0|2013-11-10 22:15:...|    70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|        0|                null|                null|       0|            null|       2|1169|\n",
      "+-----+--------------------+------+--------------------+-----+--------------------+---------+--------------------+--------------------+--------+----------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace with '0' all null values in the numVistas and nAnswers fields\n",
    "dfSE = dfSE.fillna(0, subset=[\"numViewed\", \"nAnswers\"])\n",
    "dfSE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ILZkZtlAkwd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|  id|acceptedAnswerId|\n",
      "+----+----------------+\n",
      "|1165|            null|\n",
      "|1166|            null|\n",
      "|1167|            null|\n",
      "|1168|            1170|\n",
      "|1169|            null|\n",
      "|1170|            null|\n",
      "|1171|            null|\n",
      "|1172|            null|\n",
      "|1173|            1181|\n",
      "|1174|            null|\n",
      "+----+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+----------------+\n",
      "|  id|acceptedAnswerId|\n",
      "+----+----------------+\n",
      "|1165|            null|\n",
      "|1166|            null|\n",
      "|1167|            null|\n",
      "|1168|            3000|\n",
      "|1169|            null|\n",
      "|3000|            null|\n",
      "|1171|            null|\n",
      "|1172|            null|\n",
      "|1173|            1181|\n",
      "|1174|            null|\n",
      "+----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the value 1170 with 3000 in columns \"id\" and \"acceptedAnswerId\"\n",
    "dfSE.select(\"id\", \"acceptedAnswerId\").show(10)\n",
    "dfSE.replace(1170, 3000, subset=[\"id\", \"acceptedAnswerId\"])\\\n",
    "    .select(\"id\", \"acceptedAnswerId\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0AhRhpzky7X"
   },
   "source": [
    "## Saving DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVR4GjW2k2S6"
   },
   "source": [
    "As for reading, Spark can save DateFrames in multiple formats:\n",
    "\n",
    "- CSV, JSON, Parquet, Hadoop...\n",
    "\n",
    "It can write them as well on a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EY8vG9xbk78E"
   },
   "outputs": [],
   "source": [
    "# Save the dfSE DataFrame in JSON format\n",
    "#dfSE.write.format(\"json\").mode(\"overwrite\").save(\"/content/dfSE.json\")\n",
    "dfSE.write.json(os.environ[\"DRIVE_DATA\"] + \"dfSE.json\",mode=\"overwrite\")  # save folder in repository with the possibility of over writing it\n",
    "\n",
    "#!mv /content/dfSE.json \"$DRIVE_DATA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qZdDPlv1lGzn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4M\n",
      "drwxr-xr-x 1 root root 4.0K Nov 24 13:31 .\n",
      "drwxrwxrwx 1 root root 4.0K Nov 24 13:31 ..\n",
      "-rw-r--r-- 1 root root    8 Nov 24 13:31 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 root root  11K Nov 24 13:31 .part-00000-a415ab7e-1e5f-435c-bfbd-779e64554f13-c000.json.crc\n",
      "-rw-r--r-- 1 root root    0 Nov 24 13:31 _SUCCESS\n",
      "-rw-r--r-- 1 root root 1.4M Nov 24 13:31 part-00000-a415ab7e-1e5f-435c-bfbd-779e64554f13-c000.json\n"
     ]
    }
   ],
   "source": [
    "!ls -alh \"$DRIVE_DATA\"/dfSE.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "P9Q-63hElJYQ"
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame using Parquet\n",
    "dfSE.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(os.environ[\"DRIVE_DATA\"] + \"dfSE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "cp6xMAnLlMJU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 624K\n",
      "drwxr-xr-x 1 root root 4.0K Nov 24 13:32 .\n",
      "drwxrwxrwx 1 root root 4.0K Nov 24 13:32 ..\n",
      "-rw-r--r-- 1 root root    8 Nov 24 13:32 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 root root 4.9K Nov 24 13:32 .part-00000-5a57b70c-9418-4dc6-99b3-5e03f739abac-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root    0 Nov 24 13:32 _SUCCESS\n",
      "-rw-r--r-- 1 root root 615K Nov 24 13:32 part-00000-5a57b70c-9418-4dc6-99b3-5e03f739abac-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Parquet uses by default the Snappy compressed format\n",
    "!ls -alh \"$DRIVE_DATA\"/dfSE.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPpyVB30lPd-"
   },
   "source": [
    "It will create as many files as there are partitions in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yUTF4Q8QlRhI"
   },
   "outputs": [],
   "source": [
    "dfSE2 = dfSE.repartition(2)\n",
    "# Save the DataFrame using Parquet, with gzip compression\n",
    "# Parquet is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When reading Parquet files, all columns are automatically converted to be nullable for compatibility reasons.\n",
    "dfSE2.write.format(\"parquet\")\\\n",
    "     .mode(\"overwrite\")\\\n",
    "     .option(\"compression\", \"gzip\")\\\n",
    "     .save(os.environ[\"DRIVE_DATA\"] + \"/dfSE2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yuypMiRilTaZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 424K\n",
      "drwxr-xr-x 1 root root 4.0K Nov 24 13:37 .\n",
      "drwxrwxrwx 1 root root 4.0K Nov 24 13:37 ..\n",
      "-rw-r--r-- 1 root root    8 Nov 24 13:37 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 root root 1.6K Nov 24 13:37 .part-00000-ff5ec56e-a293-454e-bf2e-6d391ba44d0c-c000.gz.parquet.crc\n",
      "-rw-r--r-- 1 root root 1.7K Nov 24 13:37 .part-00001-ff5ec56e-a293-454e-bf2e-6d391ba44d0c-c000.gz.parquet.crc\n",
      "-rw-r--r-- 1 root root    0 Nov 24 13:37 _SUCCESS\n",
      "-rw-r--r-- 1 root root 203K Nov 24 13:37 part-00000-ff5ec56e-a293-454e-bf2e-6d391ba44d0c-c000.gz.parquet\n",
      "-rw-r--r-- 1 root root 212K Nov 24 13:37 part-00001-ff5ec56e-a293-454e-bf2e-6d391ba44d0c-c000.gz.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -alh \"$DRIVE_DATA\"/dfSE2.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuQmqkp1lbU9"
   },
   "source": [
    "### Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2YPJVuWlfTB"
   },
   "source": [
    "Spark can partition and save a file using the value of a given column\n",
    "\n",
    "- A directory is created for each different value in the partitioning column\n",
    "    - All data associated to that value are stored in that directory\n",
    "- It simplifies the access to the values associated to a given key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JMXPmB-6lii6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save our DataFrame partitioned by the userID field (using Parquet)\n",
    "dfSE.write.format(\"parquet\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"userId\")\\\n",
    "    .save(os.environ[\"DRIVE_DATA\"] + \"dfSE-partition.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2HOlEv6olkyI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 root root 4.4K Nov 24 13:42 part-00000-e0845598-7893-4b65-9bd5-a4d7d6526e58.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet\n",
    "!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet/userId=10\n",
    "#rm -rf \"$DRIVE_DATA\"dfSE-partition.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2HOlEv6olkyI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 root root 4.4K Nov 24 13:42 part-00000-e0845598-7893-4b65-9bd5-a4d7d6526e58.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "#!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet\n",
    "!ls -lh \"$DRIVE_DATA\"dfSE-partition.parquet/userId=10\n",
    "#rm -rf \"$DRIVE_DATA\"dfSE-partition.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkJSOev3XOPn"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ULPx4Y1LiR"
   },
   "source": [
    "## Exercise 3.1: Word count\n",
    "\n",
    "Count the number of words *per line* in the $DRIVE_DATA/quijote.txt file. \n",
    "\n",
    "Repeat the exercise but this time counting the number of words *in the whole file*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ULPx4Y1LiR"
   },
   "source": [
    "# Exercise 3.1 Solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/20 12:29:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"DRIVE_DATA\"] = \"./data/\"\n",
    "spark = SparkSession.builder.appName(\"ex3\").getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c7Q_ljrX5RtE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|\n",
      "|                                                                           |\n",
      "|This eBook is for the use of anyone anywhere at no cost and with           |\n",
      "|almost no restrictions whatsoever.  You may copy it, give it away or       |\n",
      "|re-use it under the terms of the Project Gutenberg License included        |\n",
      "+---------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# so that we can use the F.split() function.\n",
    "\n",
    "\n",
    "file_read = spark.read.text(os.environ[\"DRIVE_DATA\"] + \"quijote.txt\")\n",
    "file_read.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------+-----+\n",
      "|lines                           |num_words|index|\n",
      "+--------------------------------+---------+-----+\n",
      "|[]                              |0        |390  |\n",
      "|[no, dirá, mofante, algu-:]     |4        |391  |\n",
      "|[Qué, don, Álvaro, de, Lu-]     |5        |392  |\n",
      "|[qué, Anibal, el, de, Carta-]   |5        |393  |\n",
      "|[qué, rey, Francisco, en, Espa-]|5        |394  |\n",
      "|[se, queja, de, la, Fortu-]     |5        |395  |\n",
      "|[Pues, al, cielo, no, le, plu-] |6        |396  |\n",
      "|[que, salieses, tan, ladi-]     |4        |397  |\n",
      "|[como, el, negro, Juan, Lati-]  |5        |398  |\n",
      "|[hablar, latines, rehú-.]       |3        |399  |\n",
      "|[No, me, despuntes, de, agu-]   |5        |400  |\n",
      "+--------------------------------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def split_count(line):\n",
    "    # if line is []:\n",
    "    #print(\"line : \", line[0])\n",
    "    #print(\"line : \", line.to)\n",
    "    return F.size(F.split(line, \" \"))\n",
    "    #return F.split(line, \" \")\n",
    "\n",
    "#num_words_per_line = file_read.select(F.regexp_extract(col(\"value\"), \"[a-z']*\", 0)).withColumn(\"num_words\", lit(split_count(F.trim(file_read.value)))).select(F.col(\"value\").alias(\"lines\"), F.col(\"num_words\"))\n",
    "#num_words_per_line.printSchema()\n",
    "#num_words_per_line.show(5, truncate=False)\n",
    "#file_read.select(F.regexp_extract(F.col(\"value\"), \"[a-z]*\", 0)).show()\n",
    "# file_read.select(F.regexp_extract(F.col(\"value\"), \"([a-zA-Z]+)\", 1)).show()\n",
    "#file_read.withColumn('lines', F.expr(r\"regexp_extract_all(value, '[a-z]*(\\\\d+)', 1)\")).show() # show numbers\n",
    "#file_read.withColumn('lines', F.expr(r\"regexp_extract_all(value, '([a-zA-Z]+)', 1)\")).show(truncate=False)\n",
    "num_words_per_line = file_read.select(F.expr(r\"regexp_extract_all(value, '([a-zA-ZÀ-ÿ]+[-_.:/@]*[a-zA-ZÀ-ÿ]*)')\").alias(\"lines\")).withColumn(\"num_words\", F.lit(F.size(F.col(\"lines\"))))\n",
    "num_words_per_line.withColumn(\"index\", F.monotonically_increasing_id()+1).filter(F.col(\"index\").between(390, 400)).show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ULPx4Y1LiR"
   },
   "source": [
    "# Exercise 3.2 Solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|sum(num_words)|\n",
      "+--------------+\n",
      "|        384233|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_words_per_line.agg({'num_words': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(num_words)=384219)]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### num_words_per_line.filter(num_words_per_line.num_words>1).select(F.sum('num_words')).collect() \n",
    "num_words_per_line.select(F.sum('num_words')).collect() \n",
    "#num_words_per_line.filter(num_words_per_line.num_words>1).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
